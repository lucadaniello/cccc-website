[
  {
    "objectID": "use-cases.html",
    "href": "use-cases.html",
    "title": "Use Cases",
    "section": "",
    "text": "Example Analyses\nHere you will find examples of how to use cccc to analyze temporal corpora (page under construction!)."
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html",
    "href": "singleFunctions/rowMassPlot.html",
    "title": "rowMassPlot()",
    "section": "",
    "text": "Visualize Keyword Frequency Distribution by Zone\nThe rowMassPlot() function creates a bar plot showing the total frequency of each keyword in your corpus, colored by frequency zone. This provides an immediate visual overview of term distribution and helps identify which terms dominate the corpus."
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#function-definition",
    "href": "singleFunctions/rowMassPlot.html#function-definition",
    "title": "rowMassPlot()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\nrowMassPlot(data)"
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#purpose",
    "href": "singleFunctions/rowMassPlot.html#purpose",
    "title": "rowMassPlot()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nUnderstanding the frequency distribution of terms in your corpus is a crucial first step in temporal analysis. The rowMassPlot() function helps you:\n\nVisualize frequency hierarchy ‚Äî See which terms are most/least frequent\nUnderstand zone distribution ‚Äî Observe how terms are distributed across frequency zones\nIdentify dominant terms ‚Äî Quickly spot high-frequency keywords\nAssess data quality ‚Äî Detect potential issues like outliers or unexpected patterns\nGuide analysis decisions ‚Äî Inform which terms to focus on for deeper analysis\n\nThis function is typically used immediately after importData() and before normalization to understand the raw frequency landscape of your corpus."
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#arguments",
    "href": "singleFunctions/rowMassPlot.html#arguments",
    "title": "rowMassPlot()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData(), containing the processed TDM (tdm), corpus metadata, zone information, and color palette."
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#plot-components",
    "href": "singleFunctions/rowMassPlot.html#plot-components",
    "title": "rowMassPlot()",
    "section": "üìä Plot Components",
    "text": "üìä Plot Components\nThe generated plot includes:\n\nX-axis\n\nKeywords ordered by total frequency (descending)\nTerms are positioned from highest to lowest frequency\n\n\n\nY-axis\n\nTotal frequency across all time periods\nRaw count of occurrences for each keyword\n\n\n\nColors\n\nEach bar is colored according to its frequency zone\nColors represent both the zone and its frequency interval\nExample labels: \"Zone 1 [0-25]\", \"Zone 4 [500-1000]\"\n\n\n\nLegend\n\nPositioned at the bottom of the plot\nShows all zone-interval combinations present in the data\nColors match those assigned during importData()"
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#output",
    "href": "singleFunctions/rowMassPlot.html#output",
    "title": "rowMassPlot()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns a ggplot2 object that can be: - Displayed directly in R - Saved to a file using ggsave() - Further customized using ggplot2 functions\nPlot characteristics: - Bar plot with keywords on x-axis - Total frequency on y-axis - Bars colored by frequency zone - Legend showing zone classifications"
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#usage-examples",
    "href": "singleFunctions/rowMassPlot.html#usage-examples",
    "title": "rowMassPlot()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage\nlibrary(cccc)\n\n# Import data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\n\n# Create frequency plot\nrowMassPlot(corpus)\n\n\nCompare Statistical vs Linguistic Zones\n# Import with statistical zones\ncorpus_stat &lt;- importData(\"tdm.csv\", \"corpus_info.csv\", zone = \"stat\")\nplot_stat &lt;- rowMassPlot(corpus_stat)\n\n# Import with linguistic zones\ncorpus_ling &lt;- importData(\"tdm.csv\", \"corpus_info.csv\", zone = \"ling\")\nplot_ling &lt;- rowMassPlot(corpus_ling)\n\n# Display side by side\nlibrary(patchwork)\nplot_stat + plot_ling + \n  plot_annotation(title = \"Comparison of Zone Classification Methods\")"
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#interpreting-the-plot",
    "href": "singleFunctions/rowMassPlot.html#interpreting-the-plot",
    "title": "rowMassPlot()",
    "section": "üîç Interpreting the Plot",
    "text": "üîç Interpreting the Plot\n\nZone Distribution Patterns\nBalanced Distribution:\nZone 1 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (many low-frequency terms)\nZone 2 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (medium-low frequency)\nZone 3 ‚ñà‚ñà‚ñà‚ñà (medium-high frequency)  \nZone 4 ‚ñà‚ñà (few high-frequency terms)\nIndicates a typical Zipfian distribution common in natural language.\nSkewed Distribution:\nZone 1 ‚ñà‚ñà (few low-frequency terms)\nZone 2 ‚ñà‚ñà‚ñà‚ñà \nZone 3 ‚ñà‚ñà‚ñà‚ñà\nZone 4 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (many high-frequency terms)\nMay indicate a specialized corpus or potential data quality issues.\n\n\nKey Observations to Look For\n\nFrequency Range:\n\nLarge gaps between zones suggest clear stratification\nSmooth transitions suggest continuous frequency distribution\n\nZone Sizes:\n\nRoughly equal zone sizes indicate balanced classification\nHighly unequal sizes may require different zone strategy\n\nOutliers:\n\nExtremely high-frequency terms may dominate the corpus\nConsider whether these should be excluded from analysis\n\nMissing Zones:\n\nAbsence of certain zones may indicate limited vocabulary range\nCommon in specialized or small corpora"
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#zone-color-schemes",
    "href": "singleFunctions/rowMassPlot.html#zone-color-schemes",
    "title": "rowMassPlot()",
    "section": "üé® Zone Color Schemes",
    "text": "üé® Zone Color Schemes\nThe plot uses colors assigned during importData():\nDefault Color Palette: - Zone 1 (lowest): Light colors (e.g., light blue, yellow) - Zone 2: Medium colors - Zone 3: Darker colors - Zone 4 (highest): Darkest colors (e.g., dark blue, red)\nColors are consistent across all cccc visualizations, allowing easy comparison between plots."
  },
  {
    "objectID": "singleFunctions/rowMassPlot.html#see-also",
    "href": "singleFunctions/rowMassPlot.html#see-also",
    "title": "rowMassPlot()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nimportData() ‚Äî Import data and assign frequency zones\ncolMassPlot() ‚Äî Visualize temporal corpus dimensions\ncurvePlot() ‚Äî Plot individual keyword trajectories\nfacetPlot() ‚Äî Create faceted visualizations by zone"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html",
    "href": "singleFunctions/optimalSmoothing.html",
    "title": "optimalSmoothing()",
    "section": "",
    "text": "Select Optimal Spline Degree and Penalization Strategy\nThe optimalSmoothing() function compares multiple smoothing strategies to identify the best combination of spline degree and penalty type. It synthesizes results from multiple smoothingSelection() runs to make a final, informed decision about optimal smoothing parameters."
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#function-definition",
    "href": "singleFunctions/optimalSmoothing.html#function-definition",
    "title": "optimalSmoothing()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\noptimalSmoothing(\n  smoothing_results,\n  plot = TRUE\n)"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#purpose",
    "href": "singleFunctions/optimalSmoothing.html#purpose",
    "title": "optimalSmoothing()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nWhile smoothingSelection() finds optimal parameters for a single penalty type, optimalSmoothing() compares multiple penalty strategies to find the globally optimal smoothing configuration.\nThis function helps you:\n\nCompare penalty strategies ‚Äî Evaluate different derivative-based penalization approaches\nMake final parameter decisions ‚Äî Select the single best combination of degree and penalty\nVisualize trade-offs ‚Äî See how different strategies perform across metrics\nEnsure robustness ‚Äî Verify that your choice is stable across different penalties\nDocument methodology ‚Äî Provide systematic justification for smoothing choices\nPrepare for clustering ‚Äî Finalize parameters before applying to all keywords\n\nThe function creates a comprehensive comparison matrix and diagnostic plots to support evidence-based parameter selection."
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#understanding-penalty-types",
    "href": "singleFunctions/optimalSmoothing.html#understanding-penalty-types",
    "title": "optimalSmoothing()",
    "section": "üßÆ Understanding Penalty Types",
    "text": "üßÆ Understanding Penalty Types\nDifferent penalty types control smoothness in different ways:\n\n‚Äúm-2‚Äù ‚Äî Adaptive Penalty (Recommended)\n\nPenalizes the (m-2)th derivative\nAdapts to spline degree automatically\nUse when: You want penalty that scales with spline complexity\nExample: For m=4, penalizes 2nd derivative (curvature)\n\n\n\n‚Äú2‚Äù ‚Äî Second Derivative Penalty\n\nAlways penalizes curvature (second derivative)\nIndependent of spline degree\nUse when: You want consistent curvature control\nExample: Prevents sharp bends in trajectories\n\n\n\n‚Äú3‚Äù ‚Äî Third Derivative Penalty\n\nPenalizes rate of curvature change (wiggliness)\nRequires m ‚â• 4\nUse when: You want to control oscillations\nExample: Smooths out rapid fluctuations\n\n\n\n‚Äú0‚Äù ‚Äî Ridge Penalty\n\nPenalizes coefficient magnitudes\nNo derivative involved\nUse when: You want simple shrinkage\nExample: Reduces overall curve amplitude"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#arguments",
    "href": "singleFunctions/optimalSmoothing.html#arguments",
    "title": "optimalSmoothing()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\nsmoothing_results\nNamed list\nrequired\nA named list where each element is the output from smoothingSelection() with a different penalty type. Names should be penalty types: \"m-2\", \"2\", \"3\", \"0\".\n\n\nplot\nLogical\nTRUE\nIf TRUE, generates comparative diagnostic plots showing GCV, OCV, df, and SSE across penalty types and spline degrees."
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#output",
    "href": "singleFunctions/optimalSmoothing.html#output",
    "title": "optimalSmoothing()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns a list with the optimal smoothing configuration and comparative diagnostics:\n\n\n\n\n\n\n\n\nElement\nType\nDescription\n\n\n\n\noptSmoothing\nlist\nComplete results from smoothingSelection() for the optimal penalty strategy. Contains all diagnostic information for the best configuration.\n\n\nm_opt\ninteger\nOptimal spline degree (m) that minimizes GCV across all penalty types and degrees.\n\n\npenalty_opt\ncharacter\nOptimal penalty type (e.g., \"m-2\", \"2\", \"3\", \"0\").\n\n\nlambda_opt\nnumeric\nOptimal log‚ÇÅ‚ÇÄ(Œª) value corresponding to the selected degree and penalty combination.\n\n\ngcv_matrix\nmatrix\nComparison matrix of GCV values with penalties as rows and degrees as columns. Lower values = better.\n\n\nplots\nlist\n(If plot = TRUE) List of ggplot2 objects showing GCV, OCV, df, and SSE comparisons.\n\n\ncall\ncall\nFunction call for reproducibility."
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#usage-examples",
    "href": "singleFunctions/optimalSmoothing.html#usage-examples",
    "title": "optimalSmoothing()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage (Two Penalties)\nlibrary(cccc)\n\n# Import and normalize data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\")\n\n# Run smoothingSelection for different penalty types\nsmooth_m2 &lt;- smoothingSelection(corpus_norm, penalty_type = \"m-2\", plot = FALSE)\nsmooth_2 &lt;- smoothingSelection(corpus_norm, penalty_type = \"2\", plot = FALSE)\n\n# Compare and select optimal\nresults &lt;- list(\"m-2\" = smooth_m2, \"2\" = smooth_2)\noptimal &lt;- optimalSmoothing(results, plot = TRUE)\n\n# View optimal parameters\ncat(\"Optimal degree:\", optimal$m_opt, \"\\n\")\ncat(\"Optimal penalty:\", optimal$penalty_opt, \"\\n\")\ncat(\"Optimal log10(lambda):\", optimal$lambda_opt, \"\\n\")\n\n\nComplete Comparison (Four Penalties)\n# Test all common penalty types\nsmooth_m2 &lt;- smoothingSelection(corpus_norm, penalty_type = \"m-2\", plot = FALSE)\nsmooth_2 &lt;- smoothingSelection(corpus_norm, penalty_type = \"2\", plot = FALSE)\nsmooth_3 &lt;- smoothingSelection(corpus_norm, penalty_type = \"3\", plot = FALSE)\nsmooth_0 &lt;- smoothingSelection(corpus_norm, penalty_type = \"0\", plot = FALSE)\n\n# Combine results\nall_results &lt;- list(\n  \"m-2\" = smooth_m2,\n  \"2\" = smooth_2,\n  \"3\" = smooth_3,\n  \"0\" = smooth_0\n)\n\n# Find optimal\noptimal &lt;- optimalSmoothing(all_results, plot = TRUE)\n\n# View GCV comparison matrix\nprint(optimal$gcv_matrix)"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#interpreting-the-output",
    "href": "singleFunctions/optimalSmoothing.html#interpreting-the-output",
    "title": "optimalSmoothing()",
    "section": "üìä Interpreting the Output",
    "text": "üìä Interpreting the Output\n\n1. GCV Comparison Matrix\noptimal$gcv_matrix\nExample output:\n        degree_2  degree_3  degree_4  degree_5\nm-2     0.0125    0.0108    0.0095    0.0102\n2       0.0131    0.0112    0.0098    0.0105\n3       0.0145    0.0120    0.0110    0.0115\nHow to read: - Rows: Penalty types - Columns: Spline degrees - Values: GCV scores (lower = better) - Minimum: Optimal combination (e.g., degree 4 with ‚Äúm-2‚Äù penalty)\n\n\n2. Diagnostic Plots\nFour comparative plots are generated (if plot = TRUE):\n\nGCV Comparison Plot\nShows GCV across degrees for each penalty type. - Find the lowest point across all lines - Check for convergence ‚Äî do different penalties agree?\n\n\nOCV Comparison Plot\nShows ordinary cross-validation scores. - Should align with GCV patterns - Discrepancies may indicate overfitting\n\n\nDegrees of Freedom Plot\nShows model complexity for each penalty. - Higher df = more flexible model - Compare at same degree ‚Äî penalties affect complexity differently\n\n\nSSE Comparison Plot\nShows fit to data for each penalty. - Lower SSE = better fit to observed data - Trade-off with smoothness ‚Äî lowest SSE may overfit"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#decision-making-guide",
    "href": "singleFunctions/optimalSmoothing.html#decision-making-guide",
    "title": "optimalSmoothing()",
    "section": "üéØ Decision-Making Guide",
    "text": "üéØ Decision-Making Guide\n\nStep 1: Examine GCV Matrix\n# Find minimum GCV\nmin_gcv &lt;- which(optimal$gcv_matrix == min(optimal$gcv_matrix), arr.ind = TRUE)\nbest_penalty &lt;- rownames(optimal$gcv_matrix)[min_gcv[1]]\nbest_degree &lt;- as.numeric(sub(\"degree_\", \"\", colnames(optimal$gcv_matrix)[min_gcv[2]]))\n\n\nStep 2: Check Plot Patterns\nGood signs: - ‚úÖ Clear minimum in GCV plot - ‚úÖ GCV and OCV agree - ‚úÖ Different penalties converge to similar degree - ‚úÖ df values are reasonable (not too extreme)\nWarning signs: - ‚ö†Ô∏è GCV keeps decreasing (no clear minimum) - ‚ö†Ô∏è Large discrepancy between GCV and OCV - ‚ö†Ô∏è Very different optimal degrees across penalties - ‚ö†Ô∏è Extremely high or low df values\n\n\nStep 3: Validate Choice\n# Use plotSuboptimalFits to visualize smoothed curves\n# (covered in next function documentation)"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#use-cases",
    "href": "singleFunctions/optimalSmoothing.html#use-cases",
    "title": "optimalSmoothing()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Final Parameter Selection\nAfter exploring with smoothingSelection(), make final decision systematically.\n\n\n2. Robustness Check\nVerify that conclusions don‚Äôt depend critically on penalty choice.\n\n\n3. Method Comparison Studies\nCompare how different penalty strategies affect results in your domain.\n\n\n4. Publication Preparation\nDocument systematic parameter selection with GCV matrix and plots.\n\n\n5. Multi-Corpus Analysis\nFind consistent smoothing strategy across different corpora."
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#tips-best-practices",
    "href": "singleFunctions/optimalSmoothing.html#tips-best-practices",
    "title": "optimalSmoothing()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nStart with 2-3 penalties ‚Äî Testing all four is often unnecessary\n‚Äúm-2‚Äù is usually best ‚Äî It adapts well to different degrees\nCheck consistency ‚Äî If penalties disagree strongly, investigate data quality\nDon‚Äôt over-optimize ‚Äî Small GCV differences (&lt; 1-2%) are not meaningful\nConsider computation ‚Äî More penalties = longer runtime\nDocument choices ‚Äî Save gcv_matrix for methods sections\nValidate visually ‚Äî Always check plots, don‚Äôt rely only on numbers\nUse practical constraints ‚Äî Degrees 3-5 cover most use cases"
  },
  {
    "objectID": "singleFunctions/optimalSmoothing.html#see-also",
    "href": "singleFunctions/optimalSmoothing.html#see-also",
    "title": "optimalSmoothing()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nsmoothingSelection() ‚Äî Find optimal Œª for each penalty (prerequisite)\nplotSuboptimalFits() ‚Äî Visualize smoothed curves with selected parameters\ncurvePlot() ‚Äî Visualize raw trajectories before smoothing\nnormalization() ‚Äî Normalize data before smoothing"
  },
  {
    "objectID": "singleFunctions/importData.html",
    "href": "singleFunctions/importData.html",
    "title": "importData()",
    "section": "",
    "text": "Import and Validate Corpus Data\nThe importData() function is the entry point for the cccc analytical pipeline. It imports, validates, and structures your corpus data and metadata into a standardized format ready for temporal analysis."
  },
  {
    "objectID": "singleFunctions/importData.html#function-definition",
    "href": "singleFunctions/importData.html#function-definition",
    "title": "importData()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\nimportData(\n  tdm_file,\n  corpus_file,\n  sep_tdm = \";\",\n  sep_corpus_info = \";\",\n  zone = \"stat\",\n  verbose = TRUE\n)"
  },
  {
    "objectID": "singleFunctions/importData.html#purpose",
    "href": "singleFunctions/importData.html#purpose",
    "title": "importData()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nThis function performs several critical operations:\n\nReads data files ‚Äî Imports term-document matrix (TDM) and corpus metadata from CSV or Excel files\nValidates structure ‚Äî Ensures data format consistency and completeness\nCleans keywords ‚Äî Standardizes term formatting and removes duplicates\nComputes frequencies ‚Äî Calculates total frequency per term across all time periods\nAssigns zones ‚Äî Classifies terms into frequency zones for stratified analysis\nStructures output ‚Äî Returns a standardized list object for downstream analysis\n\nThis function is essential for ensuring data quality and consistency before any temporal modeling or clustering operations."
  },
  {
    "objectID": "singleFunctions/importData.html#arguments",
    "href": "singleFunctions/importData.html#arguments",
    "title": "importData()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ntdm_file\nCharacter\nrequired\nPath to the term-document matrix file (CSV or Excel). First column: keywords/terms. Remaining columns: frequencies per year.\n\n\ncorpus_file\nCharacter\nrequired\nPath to the corpus information file (CSV or Excel). Must include columns for years, tokens, and number of documents per year.\n\n\nsep_tdm\nCharacter\n\";\"\nColumn separator for TDM CSV file. Ignored if file is Excel format.\n\n\nsep_corpus_info\nCharacter\n\";\"\nColumn separator for corpus info CSV file. Ignored if file is Excel format.\n\n\nzone\nCharacter\n\"stat\"\nFrequency zone classification strategy:‚Ä¢ \"stat\": Statistical quartiles‚Ä¢ \"ling\": Linguistic frequency-based zones\n\n\nverbose\nLogical\nTRUE\nIf TRUE, prints progress messages during import and processing."
  },
  {
    "objectID": "singleFunctions/importData.html#input-file-requirements",
    "href": "singleFunctions/importData.html#input-file-requirements",
    "title": "importData()",
    "section": "üìä Input File Requirements",
    "text": "üìä Input File Requirements\n\nTerm-Document Matrix (TDM) File\nThe TDM file must have the following structure:\n\n\n\nkeyword\n2000\n2001\n2002\n‚Ä¶\n\n\n\n\nalgorithm\n145\n178\n203\n‚Ä¶\n\n\ndata\n892\n945\n1021\n‚Ä¶\n\n\nnetwork\n234\n267\n289\n‚Ä¶\n\n\n\nRequirements: - First column contains keywords/terms - Subsequent columns represent years (column names should be year values) - Cell values are raw frequencies for each term in each year - Supported formats: CSV (with customizable separator) or Excel (.xlsx, .xls)\n\n\nCorpus Information File\nThe corpus metadata file must include:\n\n\n\nyear\ndimCorpus\nnDoc\n\n\n\n\n2000\n1500000\n450\n\n\n2001\n1650000\n478\n\n\n2002\n1820000\n502\n\n\n\nRequirements: - year: Year identifier (must match TDM column names) - dimCorpus: Total number of tokens in the corpus for that year - nDoc: Number of documents in the corpus for that year - Additional metadata columns are preserved but not required"
  },
  {
    "objectID": "singleFunctions/importData.html#output",
    "href": "singleFunctions/importData.html#output",
    "title": "importData()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns a list object with the following components:\n\n\n\n\n\n\n\n\nElement\nType\nDescription\n\n\n\n\ntdm\ntibble\nProcessed term-document matrix including:‚Ä¢ keyword: Cleaned term‚Ä¢ tot_freq: Total frequency across all years‚Ä¢ int_freq: Frequency interval label‚Ä¢ zone: Assigned frequency zone‚Ä¢ Year columns: Frequencies per year\n\n\ncorpus_info\ntibble\nCorpus metadata with years, dimCorpus, nDoc, and any additional metadata columns\n\n\nnorm\nlogical\nNormalization status (initially FALSE, updated after normalization())\n\n\nyear_cols\nnumeric\nVector of column indices corresponding to yearly frequencies in the TDM\n\n\nzone\ncharacter\nVector of unique frequency zone labels\n\n\ncolors\ncharacter\nDefault color palette for zones (used in visualization functions)"
  },
  {
    "objectID": "singleFunctions/importData.html#frequency-zones",
    "href": "singleFunctions/importData.html#frequency-zones",
    "title": "importData()",
    "section": "üîç Frequency Zones",
    "text": "üîç Frequency Zones\n\nStatistical Zones (zone = \"stat\")\nTerms are classified into quartile-based zones: - Zone 1: Q1 (0-25th percentile) ‚Äî Low frequency terms - Zone 2: Q2 (25-50th percentile) ‚Äî Lower-medium frequency terms - Zone 3: Q3 (50-75th percentile) ‚Äî Upper-medium frequency terms - Zone 4: Q4 (75-100th percentile) ‚Äî High frequency terms\n\n\nLinguistic Zones (zone = \"ling\")\nTerms are classified based on linguistic frequency theory: - Different thresholds based on corpus-linguistic principles - Zones reflect natural language frequency distributions - More aligned with Zipf‚Äôs law and lexical stratification"
  },
  {
    "objectID": "singleFunctions/importData.html#usage-examples",
    "href": "singleFunctions/importData.html#usage-examples",
    "title": "importData()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage\nlibrary(cccc)\n\n# Import data with default settings\ncorpus_data &lt;- importData(\n  tdm_file = \"data/term_document_matrix.csv\",\n  corpus_file = \"data/corpus_info.csv\"\n)\n\n# Check structure\nstr(corpus_data)\nnames(corpus_data$tdm)\n\n\nUsing Excel Files\n# Import from Excel files (separator arguments are ignored)\ncorpus_data &lt;- importData(\n  tdm_file = \"data/tdm.xlsx\",\n  corpus_file = \"data/corpus_metadata.xlsx\",\n  zone = \"ling\"  # Use linguistic zones\n)\n\n\nCustom Separator for CSV\n# Import CSV files with comma separator\ncorpus_data &lt;- importData(\n  tdm_file = \"data/tdm.csv\",\n  corpus_file = \"data/corpus.csv\",\n  sep_tdm = \",\",\n  sep_corpus_info = \",\",\n  verbose = TRUE\n)"
  },
  {
    "objectID": "singleFunctions/importData.html#typical-workflow",
    "href": "singleFunctions/importData.html#typical-workflow",
    "title": "importData()",
    "section": "üîó Typical Workflow",
    "text": "üîó Typical Workflow\nAfter importing data with importData(), the typical next steps are:\n\nExplore the data ‚Üí Use rowMassPlot() and colMassPlot() to visualize frequency distributions\nNormalize frequencies ‚Üí Apply normalization() to account for corpus size variations\nVisualize trajectories ‚Üí Use curvePlot() or facetPlot() to examine temporal patterns\nModel and smooth ‚Üí Apply smoothing functions to identify trends"
  },
  {
    "objectID": "singleFunctions/importData.html#see-also",
    "href": "singleFunctions/importData.html#see-also",
    "title": "importData()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nnormalization() ‚Äî Normalize the imported TDM\nrowMassPlot() ‚Äî Visualize keyword frequency distribution\ncolMassPlot() ‚Äî Visualize temporal corpus dimensions"
  },
  {
    "objectID": "singleFunctions/curvePlot.html",
    "href": "singleFunctions/curvePlot.html",
    "title": "curvePlot()",
    "section": "",
    "text": "Visualize Temporal Evolution of Keyword Frequencies\nThe curvePlot() function creates multi-line plots showing how keyword frequencies change over time. It displays all keywords in your corpus simultaneously, with colors and line weights indicating frequency zones, providing a comprehensive view of temporal dynamics."
  },
  {
    "objectID": "singleFunctions/curvePlot.html#function-definition",
    "href": "singleFunctions/curvePlot.html#function-definition",
    "title": "curvePlot()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\ncurvePlot(\n  data,\n  keywords = NULL,\n  r = 1,\n  themety = \"light\",\n  size_class = NULL,\n  x_leg = 0.85,\n  x_lab = \"years\"\n)"
  },
  {
    "objectID": "singleFunctions/curvePlot.html#purpose",
    "href": "singleFunctions/curvePlot.html#purpose",
    "title": "curvePlot()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nUnderstanding how terms evolve over time is fundamental to temporal corpus analysis. The curvePlot() function helps you:\n\nExplore temporal patterns ‚Äî See how keywords rise, fall, or remain stable over time\nIdentify trends ‚Äî Spot emerging, declining, or persistent concepts\nCompare trajectories ‚Äî Observe relationships between different keywords\nDetect events ‚Äî Find spikes or drops that correspond to historical events\nVisualize zones ‚Äî See how different frequency zones behave temporally\nAssess data quality ‚Äî Identify anomalies or irregularities in the data\nGenerate insights ‚Äî Discover unexpected patterns for further investigation\nCreate publication figures ‚Äî Produce high-quality visualizations for papers\n\nThis function can display either all keywords (comprehensive overview) or selected keywords (focused analysis), making it versatile for both exploration and presentation."
  },
  {
    "objectID": "singleFunctions/curvePlot.html#arguments",
    "href": "singleFunctions/curvePlot.html#arguments",
    "title": "curvePlot()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData() or normalization(), containing the TDM and corpus metadata.\n\n\nkeywords\nCharacter vector\nNULL\nSpecific keywords to plot. If NULL, plots all keywords in the corpus. Use this to focus on specific terms of interest.\n\n\nr\nInteger\n1\nInterval for x-axis label thinning. r = 1 shows all years, r = 2 shows every 2nd year, etc. Useful for long time series.\n\n\nthemety\nCharacter\n\"light\"\nVisual theme:‚Ä¢ \"light\": Light background (default)‚Ä¢ \"dark\": Dark background (better for presentations)\n\n\nsize_class\nNumeric vector\nNULL\nCustom line thickness for each frequency zone. If NULL, uses theme-appropriate defaults. Vector length must match number of zones.\n\n\nx_leg\nNumeric\n0.85\nHorizontal legend position (0 = left, 1 = right). Adjust if legend overlaps with curves.\n\n\nx_lab\nCharacter\n\"years\"\nLabel for x-axis (e.g., \"Years\", \"Time Period\", \"Publication Year\")."
  },
  {
    "objectID": "singleFunctions/curvePlot.html#plot-components",
    "href": "singleFunctions/curvePlot.html#plot-components",
    "title": "curvePlot()",
    "section": "üìä Plot Components",
    "text": "üìä Plot Components\n\nVisual Elements\nLines: - Each keyword is represented by one line - Line color indicates frequency zone - Line thickness can vary by zone importance\nX-axis: - Time periods (years) from corpus metadata - Labels can be thinned for long time series\nY-axis: - Frequency values (raw or normalized) - Scale depends on normalization applied\nLegend: - Shows frequency zones with corresponding colors - Position adjustable via x_leg parameter\nTheme: - Light: White background, dark text (publications) - Dark: Dark background, light text (presentations)"
  },
  {
    "objectID": "singleFunctions/curvePlot.html#usage-examples",
    "href": "singleFunctions/curvePlot.html#usage-examples",
    "title": "curvePlot()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage - All Keywords\nlibrary(cccc)\n\n# Import data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\n\n# Plot all keyword trajectories\ncurvePlot(corpus)\n\n\nPlot Specific Keywords\n# Import and normalize\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\")\n\n# Plot specific keywords of interest\ncurvePlot(\n  corpus_norm, \n  keywords = c(\"algorithm\", \"network\", \"data\", \"cloud\")\n)\n\n\nLong Time Series\n# Show every 5th year on x-axis\ncurvePlot(corpus_norm, r = 5)\n\n# Show every 10th year\ncurvePlot(corpus_norm, r = 10, x_lab = \"Decade\")"
  },
  {
    "objectID": "singleFunctions/curvePlot.html#interpreting-temporal-patterns",
    "href": "singleFunctions/curvePlot.html#interpreting-temporal-patterns",
    "title": "curvePlot()",
    "section": "üîç Interpreting Temporal Patterns",
    "text": "üîç Interpreting Temporal Patterns\n\nCommon Trajectory Types\n\n1. Emerging Terms üìà\nFrequency\n    |           ‚ï±‚îÄ‚îÄ‚îÄ\n    |         ‚ï±\n    |       ‚ï±\n    |_____‚ï±________Time\n\nStart low, increase over time\nIndicate new concepts or growing importance\nExamples: ‚Äúcloud computing‚Äù, ‚Äúdeep learning‚Äù\n\n\n\n2. Declining Terms üìâ\nFrequency\n    |‚îÄ‚îÄ‚îÄ‚ï≤\n    |     ‚ï≤\n    |       ‚ï≤\n    |         ‚ï≤___Time\n\nStart high, decrease over time\nIndicate obsolescence or reduced relevance\nExamples: ‚Äúfloppy disk‚Äù, ‚Äúmainframe‚Äù\n\n\n\n3. Stable Terms ‚û°Ô∏è\nFrequency\n    |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    |\n    |\n    |______________Time\n\nConsistent frequency over time\nIndicate core, persistent concepts\nExamples: ‚Äúcomputer‚Äù, ‚Äúdata‚Äù, ‚Äúsystem‚Äù\n\n\n\n4. Peaked Terms üèîÔ∏è\nFrequency\n    |      ‚ï±‚ï≤\n    |    ‚ï±    ‚ï≤\n    |  ‚ï±        ‚ï≤\n    |‚ï±____________‚ï≤_Time\n\nRise and fall within time period\nMay indicate temporary trends or events\nExamples: ‚ÄúY2K‚Äù, specific technology buzzwords\n\n\n\n5. Cyclical Terms üîÑ\nFrequency\n    |  ‚ï±‚ï≤    ‚ï±‚ï≤\n    | ‚ï±  ‚ï≤  ‚ï±  ‚ï≤\n    |‚ï±    ‚ï≤‚ï±    ‚ï≤_Time\n\nPeriodic fluctuations\nMay indicate recurring themes or seasonal patterns\nExamples: Conference-related terms, annual events\n\n\n\n6. Volatile Terms üìä\nFrequency\n    | ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤\n    |‚ï±        ‚ï≤Time\n\nHighly irregular patterns\nMay indicate noise or inconsistent usage\nConsider smoothing or filtering"
  },
  {
    "objectID": "singleFunctions/curvePlot.html#visual-interpretation",
    "href": "singleFunctions/curvePlot.html#visual-interpretation",
    "title": "curvePlot()",
    "section": "üé® Visual Interpretation",
    "text": "üé® Visual Interpretation\n\nZone-Based Patterns\nHigh-frequency zones (typically darker colors): - Represent core vocabulary of the corpus - Trajectories often more stable - Changes indicate major conceptual shifts\nLow-frequency zones (typically lighter colors): - Represent specialized or emerging terms - More volatile trajectories - May show sharper rises/falls\n\n\nDensity and Overlap\nDense clustering: - Many terms with similar temporal patterns - May indicate coherent topical evolution\nSpread trajectories: - Diverse temporal behaviors - Suggests heterogeneous corpus or multiple themes"
  },
  {
    "objectID": "singleFunctions/curvePlot.html#use-cases",
    "href": "singleFunctions/curvePlot.html#use-cases",
    "title": "curvePlot()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Initial Exploration\nGet a first look at temporal dynamics before detailed analysis.\n\n\n2. Hypothesis Generation\nIdentify unexpected patterns that merit further investigation.\n\n\n3. Trend Validation\nVerify expected temporal patterns (e.g., ‚Äúinternet‚Äù should rise after 1990s).\n\n\n4. Comparative Analysis\nCompare trajectories of related terms (e.g., ‚Äúanalog‚Äù vs ‚Äúdigital‚Äù).\n\n\n5. Event Detection\nFind spikes or drops corresponding to historical events.\n\n\n6. Normalization Assessment\nCompare raw vs.¬†normalized frequencies to see normalization effects.\n\n\n7. Publication Figures\nCreate visualizations for papers, presentations, or reports.\n\n\n8. Teaching/Communication\nShow collaborators or students how terms evolve in your corpus."
  },
  {
    "objectID": "singleFunctions/curvePlot.html#tips-best-practices",
    "href": "singleFunctions/curvePlot.html#tips-best-practices",
    "title": "curvePlot()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nStart with all keywords ‚Äî Get overview before focusing\nUse normalization ‚Äî Compare normalized trajectories for fair comparison\nFocus selectively ‚Äî After overview, plot specific keywords of interest\nMatch theme to context ‚Äî Light for publications, dark for presentations\nThin x-axis for long series ‚Äî Prevent label overlap (use r parameter)\nSave high resolution ‚Äî Use dpi = 300 or higher for publications\nCompare before/after normalization ‚Äî Understand normalization effects\nDocument patterns ‚Äî Keep notes on interesting trajectories for later analysis\nCross-reference with history ‚Äî Match patterns to known events/developments\nUse with other plots ‚Äî Combine with rowMassPlot() and colMassPlot() for full picture"
  },
  {
    "objectID": "singleFunctions/curvePlot.html#see-also",
    "href": "singleFunctions/curvePlot.html#see-also",
    "title": "curvePlot()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nimportData() ‚Äî Import corpus data\nnormalization() ‚Äî Normalize frequencies for fair comparison\nrowMassPlot() ‚Äî Visualize keyword frequency distribution\ncolMassPlot() ‚Äî Visualize temporal corpus structure\ncurveCtuPlot() ‚Äî Plot curves with example keywords by zone\nfacetPlot() ‚Äî Create faceted visualizations by zone"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html",
    "href": "singleFunctions/colMassPlot.html",
    "title": "colMassPlot()",
    "section": "",
    "text": "Visualize Temporal Dimensions of Corpus Structure\nThe colMassPlot() function creates a multi-line plot showing how four key corpus metrics evolve over time. This provides crucial insights into corpus growth patterns, data collection trends, and temporal coverage."
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#function-definition",
    "href": "singleFunctions/colMassPlot.html#function-definition",
    "title": "colMassPlot()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\ncolMassPlot(\n  data,\n  sc = c(1, 10, 10, 1),\n  r = 1,\n  textty = \"text\",\n  themety = \"light\",\n  size_b = 2.5,\n  x_lab = \"years\"\n)"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#purpose",
    "href": "singleFunctions/colMassPlot.html#purpose",
    "title": "colMassPlot()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nUnderstanding the temporal structure of your corpus is essential for interpreting frequency trends. The colMassPlot() function helps you:\n\nVisualize corpus growth ‚Äî See how the corpus expands over time\nIdentify collection patterns ‚Äî Detect periods of intensive/sparse data collection\nAssess temporal balance ‚Äî Evaluate whether time periods are comparably represented\nDetect anomalies ‚Äî Spot unusual spikes or drops in corpus size\nContextualize term frequencies ‚Äî Understand how corpus size affects frequency patterns\nValidate data quality ‚Äî Ensure corpus metadata is consistent and complete\n\nThis function is typically used after importData() and alongside rowMassPlot() for comprehensive data exploration before normalization."
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#arguments",
    "href": "singleFunctions/colMassPlot.html#arguments",
    "title": "colMassPlot()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData(), containing the TDM and corpus metadata.\n\n\nsc\nNumeric vector\nc(1, 10, 10, 1)\nScaling factors for the four metrics (in order):1. nDoc (number of documents)2. dimCorpus (total tokens)3. Csum (sum of keyword frequencies)4. Mcf (maximum keyword frequency)Adjust to make lines visually comparable.\n\n\nr\nInteger\n1\nInterval for x-axis label thinning. r = 2 shows every 2nd year, r = 5 shows every 5th year, etc. Useful for long time series.\n\n\ntextty\nCharacter\n\"text\"\nLabel for the unit of analysis in the legend (e.g., \"text\", \"document\", \"paper\", \"article\").\n\n\nthemety\nCharacter\n\"light\"\nVisual theme for the plot:‚Ä¢ \"light\": Light background (default)‚Ä¢ \"dark\": Dark background\n\n\nsize_b\nNumeric\n2.5\nLine thickness for the plot. Increase for bolder lines, decrease for finer lines.\n\n\nx_lab\nCharacter\n\"years\"\nLabel for the x-axis (e.g., \"Years\", \"Time Period\", \"Publication Year\")."
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#corpus-metrics-explained",
    "href": "singleFunctions/colMassPlot.html#corpus-metrics-explained",
    "title": "colMassPlot()",
    "section": "üìä Corpus Metrics Explained",
    "text": "üìä Corpus Metrics Explained\n\n1. nDoc ‚Äî Number of Documents\nThe count of documents/texts in the corpus for each time period.\nInsights: - Shows data collection intensity - Reveals archival coverage patterns - Indicates periods of high/low publication activity\n\n\n2. dimCorpus ‚Äî Total Tokens\nThe total number of words/tokens in all documents for each time period.\nInsights: - Represents overall corpus size - Important for normalization decisions - Shows writing/publication volume trends\n\n\n3. Csum ‚Äî Sum of Keyword Frequencies\nThe total frequency of all keywords across all documents in each time period.\nInsights: - Indicates keyword density in the corpus - Shows overall vocabulary coverage - Helps assess keyword selection adequacy\n\n\n4. Mcf ‚Äî Maximum Keyword Frequency\nThe highest frequency of any single keyword in each time period.\nInsights: - Identifies periods dominated by specific terms - Shows potential outliers or dominant topics - Indicates vocabulary concentration patterns"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#understanding-scaling-factors",
    "href": "singleFunctions/colMassPlot.html#understanding-scaling-factors",
    "title": "colMassPlot()",
    "section": "üé® Understanding Scaling Factors",
    "text": "üé® Understanding Scaling Factors\nDifferent metrics have vastly different scales: - nDoc might range from 10-100 - dimCorpus might range from 10,000-1,000,000 - Csum might range from 5,000-500,000 - Mcf might range from 50-5,000\nScaling makes visual comparison possible by bringing all metrics to similar ranges.\nDefault scaling: c(1, 10, 10, 1) - nDoc: √ó1 (no scaling) - dimCorpus: √ó10 (reduces by factor of 10) - Csum: √ó10 (reduces by factor of 10) - Mcf: √ó1 (no scaling)\nAdjust scaling if lines are too far apart or overlapping too much."
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#output",
    "href": "singleFunctions/colMassPlot.html#output",
    "title": "colMassPlot()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns a ggplot2 object with the following characteristics:\n\n\n\n\n\n\n\nElement\nDescription\n\n\n\n\nX-axis\nTime periods (years) from the corpus metadata\n\n\nY-axis\nRescaled values of the four corpus metrics\n\n\nLines\nFour colored lines representing each metric‚Äôs temporal evolution\n\n\nLegend\nShows metric names with scaling factors (e.g., ‚ÄúnDoc (√ó1)‚Äù, ‚ÄúdimCorpus (√ó10)‚Äù)\n\n\nTheme\nLight or dark background based on themety parameter"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#usage-examples",
    "href": "singleFunctions/colMassPlot.html#usage-examples",
    "title": "colMassPlot()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage\nlibrary(cccc)\n\n# Import data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\n\n# Create temporal plot with default settings\ncolMassPlot(corpus)"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#interpreting-the-plot",
    "href": "singleFunctions/colMassPlot.html#interpreting-the-plot",
    "title": "colMassPlot()",
    "section": "üîç Interpreting the Plot",
    "text": "üîç Interpreting the Plot\n\nCommon Patterns\n\n1. Parallel Growth\nAll four lines increase proportionally over time.\nüìà All metrics ‚ÜóÔ∏è\nInterpretation: Consistent corpus expansion with stable composition\n\n\n2. Diverging Trends\nLines separate or converge over time.\nüìà nDoc & dimCorpus ‚ÜóÔ∏è but Csum ‚ÜòÔ∏è\nInterpretation: Corpus growing but keyword density decreasing (vocabulary diversification)\n\n\n3. Spikes or Drops\nSudden changes in one or more metrics.\nüìà nDoc: sudden spike in specific year\nInterpretation: Intensive data collection period or archival event\n\n\n4. Plateau Patterns\nMetrics level off after initial growth.\nüìà Early growth ‚Üí ‚Üí ‚Üí flat\nInterpretation: Complete historical coverage or saturation"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#what-to-look-for",
    "href": "singleFunctions/colMassPlot.html#what-to-look-for",
    "title": "colMassPlot()",
    "section": "üéØ What to Look For",
    "text": "üéØ What to Look For\n\n1. Temporal Coverage\n\nAre all time periods equally represented?\nAre there gaps or sparse periods?\nDoes coverage increase toward recent years?\n\n\n\n2. Growth Patterns\n\nLinear growth (steady increase)\nExponential growth (accelerating increase)\nIrregular patterns (varying collection intensity)\n\n\n\n3. Proportionality\n\nDo nDoc and dimCorpus grow together? (expected)\nDoes Csum follow dimCorpus? (indicates keyword coverage)\nAre there periods where Mcf spikes? (term dominance)\n\n\n\n4. Anomalies\n\nSudden drops (potential data quality issues)\nIsolated spikes (special events or archival additions)\nPlateaus (collection boundaries)"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#use-cases",
    "href": "singleFunctions/colMassPlot.html#use-cases",
    "title": "colMassPlot()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Data Quality Assessment\nVerify that corpus metadata is complete and consistent across time periods.\n\n\n2. Normalization Decision\nDetermine whether normalization is necessary based on corpus size variation.\n\n\n3. Historical Context\nUnderstand how corpus collection reflects historical publication/archival patterns.\n\n\n4. Methodology Documentation\nCreate figures for research papers showing corpus characteristics.\n\n\n5. Comparative Studies\nCompare temporal structures of different corpora or subcorpora."
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#tips-best-practices",
    "href": "singleFunctions/colMassPlot.html#tips-best-practices",
    "title": "colMassPlot()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nAlways check this plot before normalization to understand corpus growth patterns\nExperiment with scaling to find the most informative visualization\nCompare with rowMassPlot() for comprehensive data exploration\nDocument temporal patterns in your methodology section\nUse r parameter for long time series (&gt;30 years) to avoid label clutter\nSave high-resolution versions for publications\nConsider normalization if you see large variations in corpus size across time"
  },
  {
    "objectID": "singleFunctions/colMassPlot.html#see-also",
    "href": "singleFunctions/colMassPlot.html#see-also",
    "title": "colMassPlot()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nimportData() ‚Äî Import corpus data and metadata\nrowMassPlot() ‚Äî Visualize keyword frequency distribution\nnormalization() ‚Äî Normalize frequencies (often needed when corpus size varies)\ncurvePlot() ‚Äî Visualize individual keyword trajectories"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The cccc package has been developed within the RIND Project - Research INtelligence Development, coordinated by the University of Trieste.\nThe RIND project is an interdisciplinary research initiative that combines methods and perspectives from computational and corpus linguistics, quantitative literary studies, data science, statistics, and translation studies.\nBuilding on Franco Moretti‚Äôs seminal approach to distant reading, the project aims to conduct large-scale quantitative analyses on a corpus of approximately 1.000 Italian novels and short story collections, including Italian translations from other languages, published between 1830 and 1930.\nThe corpus is carefully balanced to account for the distribution of authors and translators, gender, publication date, text length, and the distinction between canonical and non-canonical literature.\n\n\n\n\nRIND pursues two main research goals:\n\nAnalyzing how literary characters express thoughts and emotions, through quantitative and automatic extraction methods that capture pragma-linguistic phenomena emerging beyond the boundaries of words and sentences.\n\nTracing social change in Italian novels by exploring lexical fields related to professions, military ranks, religious titles, buildings (residential, commercial, industrial, religious, rural, urban, etc.), and other domains that reflect transformations in society.\n\nBoth objectives are innovative, as few studies have applied quantitative and computational methods to the analysis of reported discourse or to the periodization of Italian literary prose using criteria independent from traditional literary criticism.\nThe comparison between original Italian works and translations will also shed light on the influence of foreign models on Italian literary prose.\n\n\n\n\nThe project draws on three main disciplinary pillars:\n\nLinguistics, to identify morphological and syntactic features leading to pragmatic patterns and lexical lists, and to map their diachronic evolution across semantic domains.\n\nStatistics and Machine Learning, to classify texts through topic modeling, content analysis, word embeddings, time series extraction, and advanced data visualization techniques.\n\nLiterary and Comparative Studies, to interpret results within the historical and cultural contexts of Italian, European, and global literature from Realism to Modernism.\n\n\n\n\n\nBy integrating linguistic, literary, and statistical expertise, the RIND project seeks to provide both a theoretical and methodological contribution to the study of literary language and cultural evolution.\nIts results will enhance our understanding of how language and society co-evolved in Italian prose during a century of profound artistic and social transformation.\nVisit the project page: https://rind.units.it/home/"
  },
  {
    "objectID": "projects.html#the-rind-project",
    "href": "projects.html#the-rind-project",
    "title": "Projects",
    "section": "",
    "text": "The cccc package has been developed within the RIND Project - Research INtelligence Development, coordinated by the University of Trieste.\nThe RIND project is an interdisciplinary research initiative that combines methods and perspectives from computational and corpus linguistics, quantitative literary studies, data science, statistics, and translation studies.\nBuilding on Franco Moretti‚Äôs seminal approach to distant reading, the project aims to conduct large-scale quantitative analyses on a corpus of approximately 1.000 Italian novels and short story collections, including Italian translations from other languages, published between 1830 and 1930.\nThe corpus is carefully balanced to account for the distribution of authors and translators, gender, publication date, text length, and the distinction between canonical and non-canonical literature.\n\n\n\n\nRIND pursues two main research goals:\n\nAnalyzing how literary characters express thoughts and emotions, through quantitative and automatic extraction methods that capture pragma-linguistic phenomena emerging beyond the boundaries of words and sentences.\n\nTracing social change in Italian novels by exploring lexical fields related to professions, military ranks, religious titles, buildings (residential, commercial, industrial, religious, rural, urban, etc.), and other domains that reflect transformations in society.\n\nBoth objectives are innovative, as few studies have applied quantitative and computational methods to the analysis of reported discourse or to the periodization of Italian literary prose using criteria independent from traditional literary criticism.\nThe comparison between original Italian works and translations will also shed light on the influence of foreign models on Italian literary prose.\n\n\n\n\nThe project draws on three main disciplinary pillars:\n\nLinguistics, to identify morphological and syntactic features leading to pragmatic patterns and lexical lists, and to map their diachronic evolution across semantic domains.\n\nStatistics and Machine Learning, to classify texts through topic modeling, content analysis, word embeddings, time series extraction, and advanced data visualization techniques.\n\nLiterary and Comparative Studies, to interpret results within the historical and cultural contexts of Italian, European, and global literature from Realism to Modernism.\n\n\n\n\n\nBy integrating linguistic, literary, and statistical expertise, the RIND project seeks to provide both a theoretical and methodological contribution to the study of literary language and cultural evolution.\nIts results will enhance our understanding of how language and society co-evolved in Italian prose during a century of profound artistic and social transformation.\nVisit the project page: https://rind.units.it/home/"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "Function Reference",
    "section": "",
    "text": "The cccc package provides a comprehensive set of functions organized around the four-stage analytical pipeline. Below, functions are grouped by their primary purpose to help you navigate the workflow more effectively."
  },
  {
    "objectID": "functions.html#data-import-preprocessing",
    "href": "functions.html#data-import-preprocessing",
    "title": "Function Reference",
    "section": "üì• Data Import & Preprocessing",
    "text": "üì• Data Import & Preprocessing\nFunctions for importing, validating, and preparing your corpus data.\n\n\n\nFunction\nDescription\n\n\n\n\nimportData()\nImports and validates corpus data and metadata into a standardized structure for analysis.\n\n\nnormalization()\nNormalizes the term-document matrix using various schemes (nc, nchi, nM, nmM, nnl)."
  },
  {
    "objectID": "functions.html#exploratory-visualization",
    "href": "functions.html#exploratory-visualization",
    "title": "Function Reference",
    "section": "üìä Exploratory Visualization",
    "text": "üìä Exploratory Visualization\nFunctions for initial exploration and visualization of corpus characteristics.\n\n\n\nFunction\nDescription\n\n\n\n\nrowMassPlot()\nCreates a bar plot of keywords ordered by total frequency and colored by frequency zone.\n\n\ncolMassPlot()\nPlots the temporal dimensions of a corpus, showing distribution across time periods."
  },
  {
    "objectID": "functions.html#temporal-modeling-smoothing",
    "href": "functions.html#temporal-modeling-smoothing",
    "title": "Function Reference",
    "section": "üìà Temporal Modeling & Smoothing",
    "text": "üìà Temporal Modeling & Smoothing\nFunctions for modeling keyword trajectories and selecting optimal smoothing parameters.\n\n\n\nFunction\nDescription\n\n\n\n\nsmoothingSelection()\nSelects optimal smoothing parameters for chronological keyword curves using cross-validation.\n\n\noptimalSmoothing()\nSelects the optimal spline degree and penalization strategy for trajectory smoothing.\n\n\nplotSuboptimalFits()\nPlots suboptimal smoothed curves for selected keywords to compare different smoothing approaches."
  },
  {
    "objectID": "functions.html#trajectory-visualization",
    "href": "functions.html#trajectory-visualization",
    "title": "Function Reference",
    "section": "üìâ Trajectory Visualization",
    "text": "üìâ Trajectory Visualization\nFunctions for visualizing temporal curves and frequency patterns.\n\n\n\nFunction\nDescription\n\n\n\n\ncurvePlot()\nPlots temporal curves of keyword frequencies over time periods.\n\n\ncurveCtuPlot()\nPlots temporal curves for frequency zones with example keywords highlighted.\n\n\nfacetPlot()\nCreates faceted plots of keyword frequency curves for comparative visualization."
  },
  {
    "objectID": "functions.html#quick-reference-guide",
    "href": "functions.html#quick-reference-guide",
    "title": "Function Reference",
    "section": "üìñ Quick Reference Guide",
    "text": "üìñ Quick Reference Guide\n\nTypical Workflow\n\nImport your data ‚Üí importData()\nExplore the corpus ‚Üí rowMassPlot(), colMassPlot()\nNormalize frequencies ‚Üí normalization()\nVisualize trajectories ‚Üí curvePlot(), curveCtuPlot(), facetPlot()\nSelect smoothing parameters ‚Üí smoothingSelection(), optimalSmoothing()\nCompare fits ‚Üí plotSuboptimalFits()\n\n\n\nFunction Categories Summary\n\n\n\nCategory\nNumber of Functions\nPurpose\n\n\n\n\nData Import & Preprocessing\n2\nPrepare and normalize data\n\n\nExploratory Visualization\n2\nInitial corpus exploration\n\n\nTemporal Modeling\n3\nOptimize smoothing parameters\n\n\nTrajectory Visualization\n3\nVisualize temporal patterns"
  },
  {
    "objectID": "functions.html#need-help",
    "href": "functions.html#need-help",
    "title": "Function Reference",
    "section": "üîç Need Help?",
    "text": "üîç Need Help?\n\nClick on any function name to see detailed documentation\nVisit the Use Cases page for practical examples\nCheck out Projects to see cccc in action\nContact us via the About Us page for support and collaboration opportunities"
  },
  {
    "objectID": "cccc-package.html",
    "href": "cccc-package.html",
    "title": "cccc R Package",
    "section": "",
    "text": "cccc (Chronological Corpora Curve Clustering) is an innovative R package designed to analyze the temporal evolution of concepts and semantic trajectories within scientific corpora. Developed as part of the RIND Project, it provides researchers with powerful tools to understand how scientific language and knowledge evolve over time."
  },
  {
    "objectID": "cccc-package.html#the-vision",
    "href": "cccc-package.html#the-vision",
    "title": "cccc R Package",
    "section": "The Vision",
    "text": "The Vision\nIn the digital age, scientific knowledge grows exponentially, with millions of publications shaping and reshaping our understanding of the world. The cccc package was created to answer fundamental questions about this knowledge evolution:\n\nHow do scientific concepts emerge and evolve?\nWhich terms gain or lose prominence over time?\nWhat patterns characterize the life-cycle of ideas?\nHow can we map the semantic trajectories of entire research domains?\n\nBy transforming chronological corpora into living systems of evolving meanings, cccc captures how knowledge takes shape, spreads, and transforms across time periods."
  },
  {
    "objectID": "cccc-package.html#methodological-foundation",
    "href": "cccc-package.html#methodological-foundation",
    "title": "cccc R Package",
    "section": "Methodological Foundation",
    "text": "Methodological Foundation\nThe package is rooted in the paradigm of temporal scientometrics and textual dynamics modeling. It bridges:\n\nüìä Quantitative Linguistics ‚Äî Statistical analysis of language patterns\nüî¨ Computational Methods ‚Äî Advanced modeling and clustering algorithms\n\nüìö Corpus-Based Research ‚Äî Large-scale textual data analysis\nüß† Digital Humanities ‚Äî Interpretable tools for knowledge mapping\n\nThis multidisciplinary approach enables researchers to study conceptual change, topic diffusion, and knowledge transformation in ways that were previously impossible."
  },
  {
    "objectID": "cccc-package.html#core-capabilities",
    "href": "cccc-package.html#core-capabilities",
    "title": "cccc R Package",
    "section": "Core Capabilities",
    "text": "Core Capabilities\nThe cccc package implements a comprehensive analytical pipeline:\n\n1. Data Import & Preprocessing\n\nImport term-document matrices from CSV or Excel files\nClean and harmonize lexical units\nAutomatically compute frequencies and assign terms to linguistic zones\n\n\n\n2. Temporal Modeling\n\nModel word life-cycles using B-spline smoothing and penalized regression splines\nOptimize smoothing parameters through cross-validation and GCV\nVisualize raw and smoothed trajectories to assess temporal patterns\n\n\n\n3. Clustering & Analysis\n\nCluster term trajectories based on temporal profiles\nIdentify groups sharing similar growth, stability, or decline patterns\nQuantify conceptual convergence/divergence across periods\n\n\n\n4. Visualization\n\nGenerate publication-ready graphics of term dynamics\nCreate interactive and faceted visual summaries\nHighlight representative keywords and temporal peaks"
  },
  {
    "objectID": "cccc-package.html#research-applications",
    "href": "cccc-package.html#research-applications",
    "title": "cccc R Package",
    "section": "Research Applications",
    "text": "Research Applications\nWhile initially developed for literary studies, cccc extends to:\n\nScientometric Analysis ‚Äî Track emerging research themes\nBibliometric Studies ‚Äî Analyze citation and terminology trends\nSociolinguistic Research ‚Äî Study language change in social contexts\nDigital Humanities ‚Äî Explore conceptual evolution in historical texts"
  },
  {
    "objectID": "cccc-package.html#key-features",
    "href": "cccc-package.html#key-features",
    "title": "cccc R Package",
    "section": "‚öôÔ∏è Key Features",
    "text": "‚öôÔ∏è Key Features\n\nUnified interface for importing and managing temporal corpora.\n\nFlexible normalization schemes (nc, nchi, nM, nmM, nnl).\n\nAutomated smoothing parameter optimization and visualization tools.\n\nClustering of term trajectories with multiple quality indices.\n\nPublication-ready visualizations of conceptual dynamics.\n\n\n\n  \n    \n      Part of the RIND Project\n    \n    \n      cccc is developed as part of the RIND Project (Research on the INnovation Dynamics), \n      a multidisciplinary initiative combining computational linguistics, statistical modeling, and digital \n      humanities to create innovative tools for analyzing research knowledge evolution.\n    \n    \n      Learn More About RIND ‚Üí\n    \n  \n\n\n\n\n  \n    Ready to Explore Temporal Patterns in Your Corpus?\n  \n  \n    Start analyzing how concepts evolve over time with cccc\n  \n  \n    \n      View Full Documentation\n    \n    \n      Browse Functions\n    \n    \n      See Examples\n    \n  \n\n\n\ncccc turns chronological corpora into living systems of evolving meanings,\ncapturing how knowledge takes shape, spreads, and transforms over time."
  },
  {
    "objectID": "about-us.html",
    "href": "about-us.html",
    "title": "About Us",
    "section": "",
    "text": "The cccc package is developed and maintained by a team of researchers specializing in statistics, linguistics, and computational methods:"
  },
  {
    "objectID": "about-us.html#acknowledgments",
    "href": "about-us.html#acknowledgments",
    "title": "About Us",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis work is supported by the RIND Project and affiliated research institutions. We are grateful to the broader R community and the developers of key dependencies that make cccc possible."
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download and Installation",
    "section": "",
    "text": "Before installing the cccc package, you need to have R and RStudio installed on your system.\n\n\nDownload and install R from the Comprehensive R Archive Network (CRAN):\n\nWindows: Download R for Windows\nmacOS: Download R for macOS\nLinux: Download R for Linux\n\n\n\n\nRStudio is an integrated development environment (IDE) for R that makes working with R easier and more productive.\nDownload and install RStudio Desktop from Posit."
  },
  {
    "objectID": "download.html#prerequisites",
    "href": "download.html#prerequisites",
    "title": "Download and Installation",
    "section": "",
    "text": "Before installing the cccc package, you need to have R and RStudio installed on your system.\n\n\nDownload and install R from the Comprehensive R Archive Network (CRAN):\n\nWindows: Download R for Windows\nmacOS: Download R for macOS\nLinux: Download R for Linux\n\n\n\n\nRStudio is an integrated development environment (IDE) for R that makes working with R easier and more productive.\nDownload and install RStudio Desktop from Posit."
  },
  {
    "objectID": "download.html#installing-the-cccc-package",
    "href": "download.html#installing-the-cccc-package",
    "title": "Download and Installation",
    "section": "Installing the cccc Package",
    "text": "Installing the cccc Package\nCurrently, the cccc package is available exclusively on GitHub. To install it, you‚Äôll need to use the remotes package.\n\nStep 1: Install remotes\nIf you don‚Äôt already have the remotes package installed, run the following command in your R console:\ninstall.packages(\"remotes\")\n\n\nStep 2: Install cccc from GitHub\nOnce remotes is installed, you can install the cccc package directly from GitHub:\nremotes::install_github(\"matildet/cccc\")\n\n\nStep 3: Load the Package\nAfter installation, load the package to start using it:\nlibrary(cccc)"
  },
  {
    "objectID": "download.html#troubleshooting",
    "href": "download.html#troubleshooting",
    "title": "Download and Installation",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you encounter any issues during installation, please:\n\nEnsure you have the latest version of R installed\nCheck that you have a stable internet connection\nVerify that you have the necessary system dependencies for package compilation\nOpen an issue on our GitHub repository for support"
  },
  {
    "objectID": "download.html#staying-updated",
    "href": "download.html#staying-updated",
    "title": "Download and Installation",
    "section": "Staying Updated",
    "text": "Staying Updated\nSince the package is under active development, you can update to the latest version at any time by running:\nremotes::install_github(\"matildet/cccc\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Chronological Corpora Curve Clustering\n    \n      Analyze the temporal evolution of concepts and semantic trajectories in scientific corpora through advanced statistical modeling and clustering.\n    \n\n    \n      \n         Explore the Package\n      \n      \n         Get Started\n      \n    \n  \n\n\n\n\n\n  \n    Powerful Tools for Temporal Corpus Analysis\n  \n\n  \n\n    \n      üì• Data Import & Preprocessing\n      \n        Import term-document matrices, clean data, and automatically classify terms into frequency zones for stratified analysis.\n      \n    \n\n    \n      üìä Flexible Normalization\n      \n        Multiple normalization methods (nc, nchi, nM, nmM, nnl) to account for corpus size variations and enable fair temporal comparisons.\n      \n    \n\n    \n      üìà Temporal Modeling\n      \n        Model term life-cycles using B-spline smoothing with cross-validated parameter selection for optimal trajectory estimation.\n      \n    \n\n    \n      üß© Trajectory Clustering\n      \n        Cluster terms with similar temporal patterns to identify conceptual convergence, divergence, and knowledge dynamics.\n      \n    \n\n    \n      üîç Rich Visualization\n      \n        Publication-ready plots including temporal curves, faceted visualizations, and zone-based comparisons with customizable themes.\n      \n    \n\n    \n      ‚öôÔ∏è Statistical Rigor\n      \n        Grounded in temporal scientometrics with GCV/OCV criteria for parameter selection and diagnostic validation tools."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Trevisani, M., & Tuzzi, A. (2018). Chronological corpora curve clustering: From scientific corpora construction to knowledge dynamics discovery through word life-cycles clustering. MethodsX, 5, 1576-1587."
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html",
    "href": "singleFunctions/curveCtuPlot.html",
    "title": "curveCtuPlot()",
    "section": "",
    "text": "Plot Temporal Curves with Highlighted Example Keywords by Zone\nThe curveCtuPlot() function creates a temporal visualization that combines zone-based frequency curves with highlighted example keywords. This allows you to show both the overall zone structure and specific representative terms simultaneously, making it ideal for presentations and publications."
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#function-definition",
    "href": "singleFunctions/curveCtuPlot.html#function-definition",
    "title": "curveCtuPlot()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\ncurveCtuPlot(\n  data,\n  ctu_noun = NULL,\n  r = 1,\n  themety = \"light\",\n  size_class = NULL,\n  x_lab = \"years\"\n)"
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#purpose",
    "href": "singleFunctions/curveCtuPlot.html#purpose",
    "title": "curveCtuPlot()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nWhile curvePlot() shows all keywords together, curveCtuPlot() provides a more structured visualization by combining zone-level patterns with specific example keywords. This function helps you:\n\nShow zone structure ‚Äî Display aggregate patterns for each frequency zone\nHighlight representatives ‚Äî Emphasize specific keywords as examples\nCreate clear narratives ‚Äî Tell a story through selected exemplar terms\nCombine overview and detail ‚Äî Balance big picture with specific cases\nImprove readability ‚Äî Reduce clutter while maintaining informativeness\nSupport presentations ‚Äî Create clear, focused visualizations for talks\nGuide interpretation ‚Äî Direct attention to key terms within zones\nDemonstrate methodology ‚Äî Show how zone classification works with concrete examples\n\nThis function is particularly useful when you want to demonstrate temporal patterns without overwhelming your audience with hundreds of trajectories."
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#understanding-ctu-characteristic-terms-by-usage",
    "href": "singleFunctions/curveCtuPlot.html#understanding-ctu-characteristic-terms-by-usage",
    "title": "curveCtuPlot()",
    "section": "üßÆ Understanding CTU (Characteristic Terms by Usage)",
    "text": "üßÆ Understanding CTU (Characteristic Terms by Usage)\nCTU refers to selecting representative keywords from different frequency zones: - High-frequency zone: Core vocabulary terms - Medium-frequency zone: Specialized but common terms\n- Low-frequency zone: Rare or emerging terms\nBy highlighting one keyword from each zone, you show the spectrum of vocabulary in your corpus."
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#arguments",
    "href": "singleFunctions/curveCtuPlot.html#arguments",
    "title": "curveCtuPlot()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData() or normalization(), containing the TDM and corpus metadata.\n\n\nctu_noun\nCharacter vector\nNULL\nVector of exactly 3 keywords to highlight, one from each frequency zone (typically high, medium, low). Order doesn‚Äôt matter; function assigns them to zones automatically. If NULL, function may auto-select representative terms.\n\n\nr\nInteger\n1\nInterval for x-axis label thinning. r = 1 shows all years, r = 2 shows every 2nd year, etc.\n\n\nthemety\nCharacter\n\"light\"\nVisual theme:‚Ä¢ \"light\": Light background (default)‚Ä¢ \"dark\": Dark background (presentations)\n\n\nsize_class\nNumeric vector\nNULL\nCustom line thickness for the three main zones (typically high, medium, low). If NULL, uses theme-appropriate defaults. Vector length should match number of main zones.\n\n\nx_lab\nCharacter\n\"years\"\nLabel for x-axis (e.g., \"Years\", \"Time Period\", \"Publication Year\")."
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#plot-components",
    "href": "singleFunctions/curveCtuPlot.html#plot-components",
    "title": "curveCtuPlot()",
    "section": "üìä Plot Components",
    "text": "üìä Plot Components\n\nVisual Structure\nThe plot contains two layers:\n\nLayer 1: Zone Curves (Background)\n\nLines representing aggregated trajectories for each frequency zone\nColors indicate zones (using corpus color palette)\nLine thickness varies by zone importance\nShows overall zone behavior\n\n\n\nLayer 2: Example Keywords (Foreground)\n\n3 highlighted keywords from different zones\nDistinct colors separate from zone colors\nEmphasized line thickness for visibility\nLabels in legend identify specific terms\n\n\n\n\nDual Legend System\nZone Legend: - Shows frequency zones with interval ranges - Example: ‚ÄúZone 4 [500-1000]‚Äù, ‚ÄúZone 2 [50-100]‚Äù\nKeyword Legend: - Shows the 3 highlighted example keywords - Example: ‚Äúalgorithm‚Äù, ‚Äúpreprocessing‚Äù, ‚Äúnoise‚Äù"
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#usage-examples",
    "href": "singleFunctions/curveCtuPlot.html#usage-examples",
    "title": "curveCtuPlot()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage with Selected Keywords\nlibrary(cccc)\n\n# Import and normalize data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\")\n\n# Plot with 3 representative keywords from different zones\ncurveCtuPlot(\n  corpus_norm,\n  ctu_noun = c(\"algorithm\", \"preprocessing\", \"noise\")\n)\n\n\nAutomatic Keyword Selection\n# Let function auto-select representative keywords\ncurveCtuPlot(corpus_norm)\n\n\nLong Time Series\n# Show every 5th year on x-axis\ncurveCtuPlot(\n  corpus_norm,\n  ctu_noun = c(\"data\", \"network\", \"cloud\"),\n  r = 5\n)\n\n\nDark Theme for Presentations\n# Use dark theme\ncurveCtuPlot(\n  corpus_norm,\n  ctu_noun = c(\"machine\", \"learning\", \"neural\"),\n  themety = \"dark\"\n)"
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#selecting-good-example-keywords",
    "href": "singleFunctions/curveCtuPlot.html#selecting-good-example-keywords",
    "title": "curveCtuPlot()",
    "section": "üéØ Selecting Good Example Keywords",
    "text": "üéØ Selecting Good Example Keywords\n\nCriteria for CTU Selection\n1. Zone Distribution - Choose one keyword from high-frequency zone - Choose one from medium-frequency zone - Choose one from low-frequency zone\n2. Clear Trajectories - Select keywords with distinct, interpretable patterns - Avoid keywords with flat or noisy trajectories - Choose terms that tell a story\n3. Relevance - Select keywords meaningful to your research question - Choose terms your audience will recognize - Pick examples that illustrate interesting phenomena\n4. Representativeness - Keywords should be typical of their zone - Avoid extreme outliers within zones - Select terms that generalize well"
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#interpreting-the-plot",
    "href": "singleFunctions/curveCtuPlot.html#interpreting-the-plot",
    "title": "curveCtuPlot()",
    "section": "üîç Interpreting the Plot",
    "text": "üîç Interpreting the Plot\n\nUnderstanding Zone Curves\nHigh-frequency zone (typically darkest): - Shows behavior of core vocabulary - Usually more stable trajectories - Changes indicate major conceptual shifts\nMedium-frequency zone: - Shows specialized vocabulary - May show more dynamic patterns - Often contains emerging important terms\nLow-frequency zone (typically lightest): - Shows rare or specialized terms - More volatile, less stable - May contain emerging or declining concepts\n\n\nUnderstanding Highlighted Keywords\nThe 3 highlighted keywords provide concrete examples of the abstract zone patterns:\n\nHigh-freq example: Demonstrates core vocabulary evolution\nMedium-freq example: Shows specialized term dynamics\nLow-freq example: Illustrates niche or emerging concepts\n\nLook for: - Do example keywords follow their zone‚Äôs pattern? - Are there divergences that tell a story? - Do trajectories align with domain knowledge?"
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#use-cases",
    "href": "singleFunctions/curveCtuPlot.html#use-cases",
    "title": "curveCtuPlot()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Conference Presentations\nCreate clear, focused visualizations showing zone structure with examples.\n\n\n2. Publication Figures\nGenerate figures demonstrating temporal patterns across frequency strata.\n\n\n3. Teaching/Demonstrations\nShow students or colleagues how frequency zones behave differently.\n\n\n4. Methodology Validation\nDemonstrate that zone classification captures meaningful distinctions.\n\n\n5. Narrative Building\nTell a story through carefully selected representative keywords.\n\n\n6. Comparative Analysis\nCompare zone patterns across different corpora or time periods.\n\n\n7. Hypothesis Illustration\nShow specific keywords that exemplify theoretical predictions."
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#tips-best-practices",
    "href": "singleFunctions/curveCtuPlot.html#tips-best-practices",
    "title": "curveCtuPlot()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nChoose meaningful examples ‚Äî Select keywords that resonate with your audience\nTell a story ‚Äî Pick keywords that illustrate your main argument\nCheck zone membership ‚Äî Verify keywords are in different zones\nTest readability ‚Äî Ensure highlighted keywords stand out\nMatch theme to context ‚Äî Light for papers, dark for presentations\nUse consistent examples ‚Äî Keep same keywords across related plots\nDocument selection ‚Äî Explain why you chose specific keywords\nValidate patterns ‚Äî Ensure examples are representative of their zones\nSave high resolution ‚Äî Use dpi = 300+ for publications\nCombine with other plots ‚Äî Use alongside rowMassPlot() and colMassPlot()"
  },
  {
    "objectID": "singleFunctions/curveCtuPlot.html#see-also",
    "href": "singleFunctions/curveCtuPlot.html#see-also",
    "title": "curveCtuPlot()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\ncurvePlot() ‚Äî Plot all keyword trajectories without zone emphasis\nfacetPlot() ‚Äî Create faceted visualizations by zone\nrowMassPlot() ‚Äî Visualize keyword frequency distribution\ncolMassPlot() ‚Äî Visualize temporal corpus structure\nnormalization() ‚Äî Normalize frequencies before plotting"
  },
  {
    "objectID": "singleFunctions/facetPlot.html",
    "href": "singleFunctions/facetPlot.html",
    "title": "facetPlot()",
    "section": "",
    "text": "Create Faceted Visualizations of Keyword Trajectories by Zone\nThe facetPlot() function creates multi-panel plots showing keyword frequency trajectories separated by frequency zones. This faceted approach allows clear comparison of temporal patterns across different vocabulary strata without visual clutter."
  },
  {
    "objectID": "singleFunctions/facetPlot.html#function-definition",
    "href": "singleFunctions/facetPlot.html#function-definition",
    "title": "facetPlot()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\nfacetPlot(\n  data,\n  keyword_selection = list(type = \"frequency\", n = 3, kw.list = NULL),\n  r = 4,\n  scales = \"fixed\",\n  leg = TRUE,\n  themety = \"light\",\n  size_class = NULL,\n  x_lab = \"years\"\n)"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#purpose",
    "href": "singleFunctions/facetPlot.html#purpose",
    "title": "facetPlot()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nWhen visualizing temporal patterns across many keywords, overlapping trajectories can become difficult to interpret. Faceting by frequency zone provides clarity by:\n\nSeparating complexity ‚Äî Each zone displayed in its own panel\nEnabling comparisons ‚Äî Easy to compare patterns across zones\nReducing clutter ‚Äî Keywords don‚Äôt overlap across different frequency ranges\nHighlighting representatives ‚Äî Emphasize specific keywords within each zone\nSupporting analysis ‚Äî Different scales can reveal zone-specific patterns\nCreating clarity ‚Äî Clean, organized visualizations for publications\nFacilitating interpretation ‚Äî Zone-specific dynamics become immediately apparent\nFlexible selection ‚Äî Choose keywords by frequency, randomly, or manually\n\nThis function is particularly powerful for corpora with many keywords across diverse frequency ranges."
  },
  {
    "objectID": "singleFunctions/facetPlot.html#keyword-selection-strategies",
    "href": "singleFunctions/facetPlot.html#keyword-selection-strategies",
    "title": "facetPlot()",
    "section": "üßÆ Keyword Selection Strategies",
    "text": "üßÆ Keyword Selection Strategies\nThe keyword_selection parameter controls which keywords are highlighted in each zone:\n\n1. By Frequency (type = \"frequency\")\nSelects the top N most frequent keywords in each zone.\nkeyword_selection = list(type = \"frequency\", n = 3, kw.list = NULL)\nUse when: - You want to highlight the most important terms per zone - Exploring core vocabulary within each frequency stratum - Creating representative visualizations\n\n\n2. Random Selection (type = \"random\")\nRandomly samples N keywords from each zone.\nkeyword_selection = list(type = \"random\", n = 3, kw.list = NULL)\nUse when: - You want unbiased representation - Exploring general zone behavior - Testing methodology robustness\n\n\n3. Custom List (type = \"list\")\nManually specify which keywords to highlight.\nkeyword_selection = list(type = \"list\", n = NULL, kw.list = c(\"algorithm\", \"data\", \"network\"))\nUse when: - You have specific keywords of interest - Creating targeted visualizations - Following up on previous analyses"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#arguments",
    "href": "singleFunctions/facetPlot.html#arguments",
    "title": "facetPlot()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData() or normalization(), containing the TDM and corpus metadata.\n\n\nkeyword_selection\nList\nSee below\nControls keyword highlighting:‚Ä¢ type: \"frequency\", \"random\", or \"list\"‚Ä¢ n: Number of keywords per zone (for frequency/random)‚Ä¢ kw.list: Vector of specific keywords (for list type)\n\n\nr\nInteger\n4\nInterval for x-axis label thinning. Shows one label every r years.\n\n\nscales\nCharacter\n\"fixed\"\nY-axis scale behavior:‚Ä¢ \"fixed\": Same scale across all facets‚Ä¢ \"free\": Each facet has independent scale‚Ä¢ \"free_y\": Free y-axis, fixed x-axis\n\n\nleg\nLogical\nTRUE\nIf TRUE, displays legend showing zones and highlighted keywords.\n\n\nthemety\nCharacter\n\"light\"\nVisual theme:‚Ä¢ \"light\": Light background‚Ä¢ \"dark\": Dark background\n\n\nsize_class\nNumeric vector\nNULL\nCustom line thickness for each zone. If NULL, uses theme defaults.\n\n\nx_lab\nCharacter\n\"years\"\nLabel for x-axis."
  },
  {
    "objectID": "singleFunctions/facetPlot.html#understanding-scales",
    "href": "singleFunctions/facetPlot.html#understanding-scales",
    "title": "facetPlot()",
    "section": "üìä Understanding Scales",
    "text": "üìä Understanding Scales\n\nFixed Scales (scales = \"fixed\")\nZone 4 ‚îÇ [0-1000]\nZone 3 ‚îÇ [0-1000]  ‚Üê Same y-axis range\nZone 2 ‚îÇ [0-1000]\nZone 1 ‚îÇ [0-1000]\nAdvantages: - Easy to compare absolute frequencies across zones - Immediately see which zones have higher frequencies - Maintains proportional relationships\nDisadvantages: - Low-frequency zones may appear flat - Details in smaller zones harder to see\n\n\nFree Scales (scales = \"free\" or \"free_y\")\nZone 4 ‚îÇ [500-1000]\nZone 3 ‚îÇ [200-500]   ‚Üê Each zone optimized\nZone 2 ‚îÇ [50-150]\nZone 1 ‚îÇ [0-50]\nAdvantages: - Each zone‚Äôs patterns clearly visible - Reveals details in low-frequency zones - Better for pattern analysis\nDisadvantages: - Cannot compare absolute frequencies - May be misleading if not clearly labeled"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#usage-examples",
    "href": "singleFunctions/facetPlot.html#usage-examples",
    "title": "facetPlot()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage - Top Keywords per Zone\nlibrary(cccc)\n\n# Import and normalize\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\")\n\n# Create faceted plot with top 3 keywords per zone\nfacetPlot(\n  corpus_norm,\n  keyword_selection = list(type = \"frequency\", n = 3)\n)\n\n\nRandom Selection\n# Randomly select 5 keywords per zone\nfacetPlot(\n  corpus_norm,\n  keyword_selection = list(type = \"random\", n = 5)\n)\n\n\nCustom Keyword List\n# Highlight specific keywords\nfacetPlot(\n  corpus_norm,\n  keyword_selection = list(\n    type = \"list\", \n    kw.list = c(\"algorithm\", \"data\", \"network\", \"cloud\", \"machine\", \"learning\")\n  )\n)"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#interpreting-faceted-plots",
    "href": "singleFunctions/facetPlot.html#interpreting-faceted-plots",
    "title": "facetPlot()",
    "section": "üîç Interpreting Faceted Plots",
    "text": "üîç Interpreting Faceted Plots\n\nWhat to Look For in Each Panel\n\nHigh-Frequency Zone Panel\n\nCore vocabulary of the corpus\nTypically more stable trajectories\nChanges indicate major conceptual shifts\nOften shows gradual evolution rather than spikes\n\n\n\nMedium-Frequency Zone Panels\n\nSpecialized but established terms\nMay show more dynamic patterns than high-freq zone\nOften contains rising/falling terms\nBalance between stability and volatility\n\n\n\nLow-Frequency Zone Panel\n\nRare, emerging, or declining terms\nMore volatile and noisy trajectories\nSpikes may indicate temporary importance\nHarder to distinguish signal from noise\n\n\n\n\nCross-Panel Comparisons\nParallel trends across zones: - Indicates corpus-wide phenomena - May reflect historical events or methodological artifacts\nDivergent trends: - High-freq stable while low-freq volatile = normal pattern - All zones showing similar patterns = investigate common cause\nZone-specific spikes: - High-freq spike = major event (likely real signal) - Low-freq spike = may be noise or micro-trend"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#choosing-between-fixed-and-free-scales",
    "href": "singleFunctions/facetPlot.html#choosing-between-fixed-and-free-scales",
    "title": "facetPlot()",
    "section": "üéØ Choosing Between Fixed and Free Scales",
    "text": "üéØ Choosing Between Fixed and Free Scales\n\nUse Fixed Scales When:\n‚úÖ You want to compare absolute frequencies across zones\n‚úÖ Showing the magnitude difference between zones is important\n‚úÖ Creating visualizations for readers unfamiliar with faceting\n‚úÖ Emphasizing that high-freq terms dominate the corpus\nExample scenario: Demonstrating that core vocabulary (high-freq) is much more prevalent than specialized terms (low-freq).\n\n\nUse Free Scales When:\n‚úÖ You want to see patterns within each zone clearly\n‚úÖ Low-frequency zones would be too flat with fixed scales\n‚úÖ Analyzing temporal dynamics rather than absolute frequencies\n‚úÖ Looking for similar patterns across different scales\nExample scenario: Identifying whether low-frequency emerging terms show similar temporal patterns to established high-frequency terms.\n‚ö†Ô∏è Important: Always clearly label when using free scales to avoid misleading readers!"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#use-cases",
    "href": "singleFunctions/facetPlot.html#use-cases",
    "title": "facetPlot()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Zone Comparison Studies\nCompare how different frequency zones evolve over time.\n\n\n2. Pattern Discovery\nIdentify zone-specific temporal patterns not visible in combined plots.\n\n\n3. Publication Figures\nCreate clear, organized visualizations for papers showing zone structure.\n\n\n4. Presentation Material\nUse free scales to show patterns clearly to audiences.\n\n\n5. Data Exploration\nSystematically examine keywords across frequency strata.\n\n\n6. Validation\nVerify that zone classification captures meaningful differences.\n\n\n7. Hypothesis Testing\nTest whether predicted patterns appear in expected zones."
  },
  {
    "objectID": "singleFunctions/facetPlot.html#tips-best-practices",
    "href": "singleFunctions/facetPlot.html#tips-best-practices",
    "title": "facetPlot()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nStart with frequency selection ‚Äî See most important terms first\nTry both scale types ‚Äî Compare fixed vs.¬†free to understand your data\nUse appropriate n ‚Äî 2-4 keywords per zone usually optimal\nSave large versions ‚Äî Faceted plots need more space than single plots\nLabel clearly ‚Äî Especially important when using free scales\nMatch theme to context ‚Äî Light for papers, dark for talks\nConsider zone count ‚Äî More zones = need more vertical space\nTest keyword selection ‚Äî Try random selection to verify representativeness\nDocument choices ‚Äî Note which keywords and why in methods\nCombine with other plots ‚Äî Use alongside curveCtuPlot() for comprehensive view"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#see-also",
    "href": "singleFunctions/facetPlot.html#see-also",
    "title": "facetPlot()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\ncurvePlot() ‚Äî Plot all keywords together without faceting\ncurveCtuPlot() ‚Äî Zone curves with highlighted examples (alternative approach)\nrowMassPlot() ‚Äî Visualize frequency distribution by zone\nnormalization() ‚Äî Normalize before plotting for fair comparison\nimportData() ‚Äî Zone classification happens here"
  },
  {
    "objectID": "singleFunctions/facetPlot.html#need-help",
    "href": "singleFunctions/facetPlot.html#need-help",
    "title": "facetPlot()",
    "section": "üí¨ Need Help?",
    "text": "üí¨ Need Help?\nFor questions about faceted visualizations: - Check the Use Cases page for complete workflow examples - Visit the Projects page to see real applications - Open an issue on GitHub - Contact the team via the About Us page"
  },
  {
    "objectID": "singleFunctions/normalization.html",
    "href": "singleFunctions/normalization.html",
    "title": "normalization()",
    "section": "",
    "text": "Normalize Term-Document Matrix for Temporal Analysis\nThe normalization() function standardizes raw keyword frequencies to enable meaningful comparisons across time periods with varying corpus sizes. It applies one of five normalization strategies, each suited to different analytical goals."
  },
  {
    "objectID": "singleFunctions/normalization.html#function-definition",
    "href": "singleFunctions/normalization.html#function-definition",
    "title": "normalization()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\nnormalization(\n  data,\n  normty = \"nc\",\n  sc = 1000,\n  nnlty = \"V\",\n  p_asy = TRUE\n)"
  },
  {
    "objectID": "singleFunctions/normalization.html#purpose",
    "href": "singleFunctions/normalization.html#purpose",
    "title": "normalization()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nRaw frequency counts are influenced by corpus size variations across time periods. A term appearing 100 times in a corpus of 10,000 words is much more significant than the same count in a corpus of 1,000,000 words.\nThe normalization() function addresses this by:\n\nAdjusting for corpus size ‚Äî Accounts for different document/token counts per year\nEnabling fair comparisons ‚Äî Makes frequencies comparable across time periods\nHighlighting relative importance ‚Äî Emphasizes terms‚Äô relative prominence\nPreparing for modeling ‚Äî Creates standardized data for temporal smoothing and clustering\n\nThis function is typically applied after importData() and before temporal modeling and visualization."
  },
  {
    "objectID": "singleFunctions/normalization.html#arguments",
    "href": "singleFunctions/normalization.html#arguments",
    "title": "normalization()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData(), containing the TDM and corpus metadata.\n\n\nnormty\nCharacter\n\"nc\"\nNormalization method to apply. Options:‚Ä¢ \"nc\": Column normalization by corpus size‚Ä¢ \"nchi\": Chi-square-like normalization‚Ä¢ \"nM\": Maximum frequency normalization‚Ä¢ \"nmM\": Min-max normalization‚Ä¢ \"nnl\": Non-linear normalization\n\n\nsc\nNumeric\n1000\nScaling factor applied after normalization. Default is 1000 for \"nc\" and \"nM\", otherwise 1.\n\n\nnnlty\nCharacter\n\"V\"\nAsymmetry measure for non-linear normalization (only used when normty = \"nnl\"):‚Ä¢ \"V\": Variance-based asymmetry‚Ä¢ \"M\": Mean-median-based asymmetry\n\n\np_asy\nLogical\nTRUE\nIf TRUE and normty = \"nnl\", includes asymmetry coefficients in the output."
  },
  {
    "objectID": "singleFunctions/normalization.html#normalization-methods",
    "href": "singleFunctions/normalization.html#normalization-methods",
    "title": "normalization()",
    "section": "üìä Normalization Methods",
    "text": "üìä Normalization Methods\n\n1. Column Normalization (\"nc\")\nFormula: Normalized frequency = (raw frequency / total tokens in year) √ó scaling factor\nUse when: - You want to account for varying corpus sizes across time periods - Comparing frequencies across years with different document counts - Standard normalization for most temporal analyses\nExample:\ncorpus_norm &lt;- normalization(corpus_data, normty = \"nc\", sc = 1000)\nThis converts raw frequencies to ‚Äúper 1000 tokens‚Äù rates.\n\n\n\n2. Chi-square Normalization (\"nchi\")\nFormula: Based on chi-square decomposition using row masses and expected frequencies\nUse when: - You want to emphasize deviations from expected frequencies - Performing correspondence analysis-style normalization - Highlighting terms that appear more/less than expected\nExample:\ncorpus_norm &lt;- normalization(corpus_data, normty = \"nchi\")\nThis method is rooted in correspondence analysis theory and emphasizes relative contributions.\n\n\n\n3. Maximum Frequency Normalization (\"nM\")\nFormula: Normalized frequency = (raw frequency / maximum frequency in row) √ó scaling factor\nUse when: - You want to focus on relative peaks within each term‚Äôs trajectory - Comparing temporal patterns regardless of absolute frequency - All terms scaled to same maximum (useful for clustering)\nExample:\ncorpus_norm &lt;- normalization(corpus_data, normty = \"nM\", sc = 1000)\nEach term‚Äôs maximum frequency becomes the scaling reference point.\n\n\n\n4. Min-Max Normalization (\"nmM\")\nFormula: Normalized frequency = (raw frequency - min) / (max - min)\nUse when: - You need all values scaled to [0, 1] range - Comparing terms with vastly different frequency ranges - Preparing data for specific clustering algorithms\nExample:\ncorpus_norm &lt;- normalization(corpus_data, normty = \"nmM\")\nAll normalized values fall between 0 (minimum) and 1 (maximum) for each term.\n\n\n\n5. Non-linear Normalization (\"nnl\")\nFormula: Incorporates asymmetry coefficients based on distribution shape\nUse when: - Term frequencies show strong skewness or asymmetry - You want to account for variance or mean-median differences - Advanced modeling requires distribution-aware normalization\nAsymmetry types: - \"V\": Variance-based (accounts for spread) - \"M\": Mean-median-based (accounts for skewness)\nExample:\n# Variance-based\ncorpus_norm &lt;- normalization(corpus_data, normty = \"nnl\", nnlty = \"V\")\n\n# Mean-median-based\ncorpus_norm &lt;- normalization(corpus_data, normty = \"nnl\", nnlty = \"M\", p_asy = TRUE)"
  },
  {
    "objectID": "singleFunctions/normalization.html#output",
    "href": "singleFunctions/normalization.html#output",
    "title": "normalization()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns a list with the same structure as input, with these updates:\n\n\n\n\n\n\n\n\nElement\nType\nDescription\n\n\n\n\ntdm\ntibble\nTerm-document matrix with normalized frequencies\n\n\ncorpus_info\ntibble\nUnchanged corpus metadata\n\n\nnorm\nlogical\nSet to TRUE indicating normalization has been applied\n\n\nnormty\ncharacter\nThe normalization method used\n\n\nyear_cols\nnumeric\nUnchanged column indices for yearly data\n\n\nzone\ncharacter\nUnchanged frequency zones\n\n\ncolors\ncharacter\nUnchanged color palette\n\n\np_asy\nnumeric\n(Optional) Asymmetry coefficients (only for \"nnl\" method with p_asy = TRUE)"
  },
  {
    "objectID": "singleFunctions/normalization.html#usage-examples",
    "href": "singleFunctions/normalization.html#usage-examples",
    "title": "normalization()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Column Normalization\nlibrary(cccc)\n\n# Import data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\n\n# Apply standard column normalization (per 1000 tokens)\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\", sc = 1000)\n\n# Check normalization status\ncorpus_norm$norm  # Should be TRUE\ncorpus_norm$normty  # Should be \"nc\"\n\n\nComparing Different Methods\n# Column normalization\ncorpus_nc &lt;- normalization(corpus, normty = \"nc\")\n\n# Chi-square normalization\ncorpus_nchi &lt;- normalization(corpus, normty = \"nchi\")\n\n# Max normalization\ncorpus_nM &lt;- normalization(corpus, normty = \"nM\")\n\n# Compare visualizations\ncurvePlot(corpus_nc, keywords = c(\"algorithm\", \"data\"))\ncurvePlot(corpus_nchi, keywords = c(\"algorithm\", \"data\"))\n\n\nNon-linear Normalization with Asymmetry\n# Apply non-linear normalization with variance-based asymmetry\ncorpus_nnl &lt;- normalization(\n  corpus, \n  normty = \"nnl\", \n  nnlty = \"V\", \n  p_asy = TRUE\n)\n\n# View asymmetry coefficients\nhead(corpus_nnl$p_asy)\n\n\nCustom Scaling Factor\n# Normalize to \"per 10,000 tokens\" instead of per 1000\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\", sc = 10000)"
  },
  {
    "objectID": "singleFunctions/normalization.html#choosing-the-right-method",
    "href": "singleFunctions/normalization.html#choosing-the-right-method",
    "title": "normalization()",
    "section": "üìä Choosing the Right Method",
    "text": "üìä Choosing the Right Method\n\n\n\n\n\n\n\n\n\nMethod\nBest For\nPros\nCons\n\n\n\n\nnc\nGeneral temporal analysis\nSimple, interpretable\nMay not handle extreme skewness well\n\n\nnchi\nDeviation-focused analysis\nEmphasizes unexpected patterns\nLess intuitive interpretation\n\n\nnM\nPattern comparison\nGood for clustering\nLoses absolute frequency information\n\n\nnmM\nAlgorithm preparation\nBounded [0,1] range\nSensitive to outliers\n\n\nnnl\nAsymmetric distributions\nAccounts for skewness\nMore complex, requires understanding\n\n\n\nRecommendation: Start with \"nc\" for most analyses. Use \"nchi\" for correspondence-style analysis, \"nM\" for clustering, and \"nnl\" for distributions with strong asymmetry."
  },
  {
    "objectID": "singleFunctions/normalization.html#typical-workflow",
    "href": "singleFunctions/normalization.html#typical-workflow",
    "title": "normalization()",
    "section": "üîó Typical Workflow",
    "text": "üîó Typical Workflow\n# 1. Import data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\n\n# 2. Explore raw frequencies\nrowMassPlot(corpus)\ncolMassPlot(corpus)\n\n# 3. Normalize\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\", sc = 1000)\n\n# 4. Visualize normalized trajectories\ncurvePlot(corpus_norm, keywords = c(\"algorithm\", \"network\", \"data\"))\nfacetPlot(corpus_norm, zone = \"all\")\n\n# 5. Proceed with smoothing\nsmooth_params &lt;- smoothingSelection(corpus_norm)"
  },
  {
    "objectID": "singleFunctions/normalization.html#important-notes",
    "href": "singleFunctions/normalization.html#important-notes",
    "title": "normalization()",
    "section": "‚ö†Ô∏è Important Notes",
    "text": "‚ö†Ô∏è Important Notes\n\nRe-normalization\nOnce a dataset is normalized (norm = TRUE), applying normalization() again will normalize the already-normalized data. Always start from raw data if you need to try different normalization methods.\n\n\nZero Frequencies\nTerms with zero frequencies in all time periods will remain zero after normalization. Consider filtering these out before normalization if needed.\n\n\nInterpretation\nAfter normalization, frequency values no longer represent raw counts. Always specify the normalization method and scaling factor when reporting results."
  },
  {
    "objectID": "singleFunctions/normalization.html#see-also",
    "href": "singleFunctions/normalization.html#see-also",
    "title": "normalization()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nimportData() ‚Äî Import and prepare data (required before normalization)\ncurvePlot() ‚Äî Visualize normalized temporal curves\nfacetPlot() ‚Äî Compare normalized trajectories across zones\nsmoothingSelection() ‚Äî Next step: select smoothing parameters"
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html",
    "href": "singleFunctions/plotSuboptimalFits.html",
    "title": "plotSuboptimalFits()",
    "section": "",
    "text": "Visualize Smoothed Curves for Quality Assessment\nThe plotSuboptimalFits() function creates visual comparisons of raw and smoothed keyword frequency trajectories. It helps you assess how well your chosen smoothing parameters perform across different types of keywords in your corpus."
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#function-definition",
    "href": "singleFunctions/plotSuboptimalFits.html#function-definition",
    "title": "plotSuboptimalFits()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\nplotSuboptimalFits(\n  data,\n  opt_res,\n  n_curves = 9,\n  show_zone = FALSE,\n  graph = FALSE\n)"
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#purpose",
    "href": "singleFunctions/plotSuboptimalFits.html#purpose",
    "title": "plotSuboptimalFits()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nAfter selecting optimal smoothing parameters with optimalSmoothing(), it‚Äôs crucial to visually validate that the smoothing works well across your entire corpus. This function helps you:\n\nAssess smoothing quality ‚Äî See how well smoothed curves capture underlying trends\nDetect overfitting/undersmoothing ‚Äî Identify cases where smoothing is too aggressive or too weak\nEvaluate representativeness ‚Äî Check performance across different keyword types\nCompare raw vs.¬†smoothed ‚Äî Understand what information is retained vs.¬†filtered\nIdentify problematic cases ‚Äî Find keywords that may need special treatment\nBuild confidence ‚Äî Validate that parameters work well before applying to full corpus\nCreate publication figures ‚Äî Generate high-quality visualizations of smoothing results\n\nThe function intelligently samples keywords across the residual distribution to show a representative range of smoothing performance."
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#how-it-works",
    "href": "singleFunctions/plotSuboptimalFits.html#how-it-works",
    "title": "plotSuboptimalFits()",
    "section": "üßÆ How It Works",
    "text": "üßÆ How It Works\n\nRMS Residual Sampling\nThe function computes Root Mean Square (RMS) residuals for all keywords:\nRMS = ‚àö(Œ£(observed - smoothed)¬≤ / n)\nThen selects n_curves keywords distributed across the RMS range: - Low RMS: Keywords where smoothing fits very well - Medium RMS: Typical smoothing performance - High RMS: Keywords with more complex patterns or poor fits\nThis ensures you see the full spectrum of smoothing behavior, not just the best cases."
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#arguments",
    "href": "singleFunctions/plotSuboptimalFits.html#arguments",
    "title": "plotSuboptimalFits()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData() or normalization(), containing the TDM and corpus metadata.\n\n\nopt_res\nList\nrequired\nThe optimal smoothing configuration returned by optimalSmoothing(), including spline degree (m_opt), penalty type (penalty_opt), and lambda (lambda_opt).\n\n\nn_curves\nInteger\n9\nNumber of keywords to visualize. Must be a perfect square for optimal grid layout (e.g., 4, 9, 16, 25).\n\n\nshow_zone\nLogical\nFALSE\nIf TRUE, includes the keyword‚Äôs frequency zone in plot titles (e.g., ‚Äúalgorithm [Zone 4]‚Äù).\n\n\ngraph\nLogical\nFALSE\nIf TRUE, displays plots immediately in the R graphics device. If FALSE (default), plots are returned invisibly and can be accessed from the output list."
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#output",
    "href": "singleFunctions/plotSuboptimalFits.html#output",
    "title": "plotSuboptimalFits()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns (invisibly) a list containing visualization objects:\n\n\n\n\n\n\n\n\nElement\nType\nDescription\n\n\n\n\nsingleKeywordPlot\nlist\nA list of individual ggplot2 objects, one for each selected keyword. Each plot shows raw (dashed) and smoothed (solid) curves with keyword name in title.\n\n\ncombinedKeywordPlot\npatchwork\nA combined grid layout displaying all selected keyword plots together. Uses patchwork package for arrangement.\n\n\n\nPlot characteristics: - Grey dashed line: Raw frequency trajectory - Red solid line: Smoothed spline fit - X-axis: Time periods (years) - Y-axis: Frequency (raw or normalized, depending on input data) - Title: Keyword name (and zone if show_zone = TRUE)"
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#usage-examples",
    "href": "singleFunctions/plotSuboptimalFits.html#usage-examples",
    "title": "plotSuboptimalFits()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage\nlibrary(cccc)\n\n# Complete workflow\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\")\n\n# Find optimal parameters\nsmooth_m2 &lt;- smoothingSelection(corpus_norm, penalty_type = \"m-2\", plot = FALSE)\nsmooth_2 &lt;- smoothingSelection(corpus_norm, penalty_type = \"2\", plot = FALSE)\noptimal &lt;- optimalSmoothing(list(\"m-2\" = smooth_m2, \"2\" = smooth_2))\n\n# Visualize smoothing quality\nfits &lt;- plotSuboptimalFits(corpus_norm, optimal)\n\n# Display combined plot\nfits$combinedKeywordPlot\n\n\nShow Individual Plots\n# Create plots\nfits &lt;- plotSuboptimalFits(corpus_norm, optimal, n_curves = 9)\n\n# View first individual plot\nfits$singleKeywordPlot[[1]]\n\n# View specific keyword plot\nfits$singleKeywordPlot[[5]]\n\n# Save individual plots\nlibrary(ggplot2)\nggsave(\"keyword1_fit.png\", fits$singleKeywordPlot[[1]], width = 8, height = 5)\n\n\nInclude Zone Information\n# Add frequency zone to titles\nfits &lt;- plotSuboptimalFits(\n  corpus_norm, \n  optimal, \n  n_curves = 9,\n  show_zone = TRUE\n)\n\n# Now titles show: \"algorithm [Zone 4]\"\nfits$combinedKeywordPlot\n\n\nMore/Fewer Keywords\n# Show 4 keywords (2√ó2 grid)\nfits_small &lt;- plotSuboptimalFits(corpus_norm, optimal, n_curves = 4)\n\n# Show 16 keywords (4√ó4 grid)\nfits_large &lt;- plotSuboptimalFits(corpus_norm, optimal, n_curves = 16)\n\n# Show 25 keywords (5√ó5 grid)\nfits_xlarge &lt;- plotSuboptimalFits(corpus_norm, optimal, n_curves = 25)"
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#interpreting-the-plots",
    "href": "singleFunctions/plotSuboptimalFits.html#interpreting-the-plots",
    "title": "plotSuboptimalFits()",
    "section": "üîç Interpreting the Plots",
    "text": "üîç Interpreting the Plots\n\nWhat to Look For\n\n‚úÖ Good Smoothing\nRaw:     ‚Ä¢ ‚Ä¢ ‚Ä¢  ‚Ä¢  ‚Ä¢\n           ‚ï±‚ï≤  ‚ï±‚ï≤\nSmooth: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  (captures trend, reduces noise)\n\nSmoothed curve follows general trend\nReduces noise without losing important features\nNo systematic bias (doesn‚Äôt consistently over/underestimate)\n\n\n\n‚ö†Ô∏è Oversmoothing\nRaw:     ‚Ä¢ ‚Ä¢ ‚Ä¢  ‚Ä¢  ‚Ä¢\n           ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤\nSmooth: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (too flat)\n\nSmoothed curve misses important peaks or valleys\nTrajectory appears unnaturally flat\nReal patterns are suppressed\n\n\n\n‚ö†Ô∏è Undersmoothing\nRaw:     ‚Ä¢ ‚Ä¢ ‚Ä¢  ‚Ä¢  ‚Ä¢\n           ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤\nSmooth:   ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤  (too wiggly)\n\nSmoothed curve follows noise too closely\nTrajectory has spurious fluctuations\nFails to reveal underlying trend\n\n\n\n‚ö†Ô∏è Systematic Bias\nRaw:     ‚Ä¢ ‚Ä¢ ‚Ä¢  ‚Ä¢  ‚Ä¢\n         ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤\nSmooth: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (consistently below/above)\n\nSmoothed curve consistently over- or underestimates\nMay indicate inappropriate penalty or normalization"
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#understanding-the-selection",
    "href": "singleFunctions/plotSuboptimalFits.html#understanding-the-selection",
    "title": "plotSuboptimalFits()",
    "section": "üìä Understanding the Selection",
    "text": "üìä Understanding the Selection\n\nRMS Distribution Sampling\nIf you have 1000 keywords and request n_curves = 9, the function:\n\nComputes RMS for all 1000 keywords\nSorts keywords by RMS (low to high)\nSamples 9 keywords evenly across the distribution:\n\nKeywords ~1, 125, 250, 375, 500, 625, 750, 875, 1000\n\n\nThis gives you: - Low RMS keywords: Best-fit cases (smooth, predictable) - Medium RMS keywords: Typical cases (moderate complexity) - High RMS keywords: Challenging cases (noisy, volatile)\n\n\nWhy This Matters\nSeeing only low-RMS keywords would give false confidence. Seeing only high-RMS keywords would be unnecessarily discouraging. The representative sample shows you the realistic range of smoothing performance."
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#use-cases",
    "href": "singleFunctions/plotSuboptimalFits.html#use-cases",
    "title": "plotSuboptimalFits()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Quality Assurance\nBefore applying smoothing to full corpus, verify it works well.\n\n\n2. Parameter Validation\nConfirm that optimalSmoothing() choices are actually optimal visually.\n\n\n3. Method Comparison\nCompare smoothing with different parameters side-by-side.\n\n\n4. Publication Figures\nCreate figures showing smoothing effectiveness for methods sections.\n\n\n5. Identifying Outliers\nFind keywords with unusual temporal patterns that need special attention.\n\n\n6. Training Examples\nShow collaborators/reviewers how smoothing works on your data."
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#tips-best-practices",
    "href": "singleFunctions/plotSuboptimalFits.html#tips-best-practices",
    "title": "plotSuboptimalFits()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nAlways run this function ‚Äî Don‚Äôt skip visual validation\nUse perfect squares for n_curves (4, 9, 16, 25) for clean grid layouts\nStart with 9 ‚Äî Good balance between coverage and readability\nCheck high-RMS cases ‚Äî If they look terrible, reconsider parameters\nSave the plots ‚Äî Include in supplementary materials or methods sections\nShow to colleagues ‚Äî Get feedback on whether smoothing looks reasonable\nDon‚Äôt expect perfection ‚Äî Some keywords will always be noisy\nCompare normalizations ‚Äî Try different normalization methods if smoothing looks poor"
  },
  {
    "objectID": "singleFunctions/plotSuboptimalFits.html#see-also",
    "href": "singleFunctions/plotSuboptimalFits.html#see-also",
    "title": "plotSuboptimalFits()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\noptimalSmoothing() ‚Äî Select parameters (prerequisite for this function)\nsmoothingSelection() ‚Äî Find optimal Œª for penalties\ncurvePlot() ‚Äî Visualize specific keyword trajectories\nfacetPlot() ‚Äî Create faceted visualizations by zone"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html",
    "href": "singleFunctions/smoothingSelection.html",
    "title": "smoothingSelection()",
    "section": "",
    "text": "Select Optimal Smoothing Parameters for Temporal Curves\nThe smoothingSelection() function identifies the best smoothing parameters for modeling keyword frequency trajectories over time. It systematically evaluates different combinations of spline degrees and smoothing penalties to find the optimal balance between smoothness and fidelity to the data."
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#function-definition",
    "href": "singleFunctions/smoothingSelection.html#function-definition",
    "title": "smoothingSelection()",
    "section": "üîπ Function Definition",
    "text": "üîπ Function Definition\nsmoothingSelection(\n  data,\n  lambda_seq = NULL,\n  degrees = NULL,\n  penalty_type = \"m-2\",\n  normty = NULL,\n  plot = TRUE,\n  verbose = TRUE\n)"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#purpose",
    "href": "singleFunctions/smoothingSelection.html#purpose",
    "title": "smoothingSelection()",
    "section": "üéØ Purpose",
    "text": "üéØ Purpose\nRaw keyword frequency curves are often noisy and irregular, making it difficult to identify underlying temporal trends. Smoothing helps reveal the true signal by reducing noise while preserving meaningful patterns.\nThe smoothingSelection() function helps you:\n\nFind optimal smoothing level ‚Äî Determine how much smoothing is appropriate for your data\nSelect spline complexity ‚Äî Choose the right degree of B-spline for modeling\nBalance fit vs.¬†smoothness ‚Äî Avoid both overfitting (too wiggly) and oversmoothing (too flat)\nCompare penalty strategies ‚Äî Evaluate different derivative-based penalization approaches\nMake informed decisions ‚Äî Use diagnostic criteria (GCV, OCV) for objective parameter selection\nVisualize trade-offs ‚Äî See how different parameters affect model quality\n\nThis function uses penalized B-spline regression with cross-validation to find parameters that generalize well to unseen data."
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#statistical-background",
    "href": "singleFunctions/smoothingSelection.html#statistical-background",
    "title": "smoothingSelection()",
    "section": "üßÆ Statistical Background",
    "text": "üßÆ Statistical Background\n\nB-splines\nB-splines (basis splines) are piecewise polynomial functions that provide flexible curve fitting: - Degree (m): Controls the polynomial order (linear, quadratic, cubic, etc.) - Higher degrees = smoother, more flexible curves - Lower degrees = simpler, more constrained curves\n\n\nSmoothing Parameter (Œª)\nControls the trade-off between: - Fidelity to data (small Œª): Curve follows data points closely but may be noisy - Smoothness (large Œª): Curve is smooth but may miss important features\n\n\nPenalty Types\n\n‚Äúm-2‚Äù: Penalizes the (m-2)th derivative (default, adaptive to spline degree)\n‚Äú2‚Äù: Penalizes the second derivative (curvature penalty)\n‚Äú3‚Äù: Penalizes the third derivative (wiggliness penalty)\n\n\n\nDiagnostic Criteria\nGCV (Generalized Cross-Validation): - Estimates prediction error without actually splitting data - Lower GCV = better model performance - Preferred when computational efficiency matters\nOCV (Ordinary Cross-Validation): - Leave-one-out cross-validation - More computationally intensive but more accurate - Lower OCV = better generalization"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#arguments",
    "href": "singleFunctions/smoothingSelection.html#arguments",
    "title": "smoothingSelection()",
    "section": "‚öôÔ∏è Arguments",
    "text": "‚öôÔ∏è Arguments\n\n\n\n\n\n\n\n\n\nArgument\nType\nDefault\nDescription\n\n\n\n\ndata\nList\nrequired\nA list object returned by importData() or normalization(), containing the TDM and corpus metadata.\n\n\nlambda_seq\nNumeric vector\nseq(-6, 9, 0.25)\nSequence of log‚ÇÅ‚ÇÄ(Œª) values to evaluate. Range from 10‚Åª‚Å∂ to 10‚Åπ. Finer sequences provide more precision but take longer.\n\n\ndegrees\nInteger vector\n1:8\nRange of B-spline degrees (m) to test. Common choices: 3 (cubic), 4 (quartic), 5 (quintic).\n\n\npenalty_type\nCharacter\n\"m-2\"\nPenalty applied to derivatives:‚Ä¢ \"m-2\": Adaptive penalty (default)‚Ä¢ \"2\": Second derivative (curvature)‚Ä¢ \"3\": Third derivative (wiggliness)\n\n\nnormty\nCharacter\nNULL\nLabel for normalization method used (for documentation purposes). Automatically detected if data was normalized.\n\n\nplot\nLogical\nTRUE\nIf TRUE, produces diagnostic plots showing df, SSE, GCV, and OCV across Œª values.\n\n\nverbose\nLogical\nTRUE\nIf TRUE, prints progress messages for each spline degree being evaluated."
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#output",
    "href": "singleFunctions/smoothingSelection.html#output",
    "title": "smoothingSelection()",
    "section": "üì¶ Output",
    "text": "üì¶ Output\nReturns a list with comprehensive smoothing diagnostics:\n\n\n\n\n\n\n\n\nElement\nType\nDescription\n\n\n\n\nresults\ndata.frame\nComplete grid of all (m, Œª) combinations with diagnostic measures (df, sse, gcv, ocv).\n\n\nsummary_optimal\ndata.frame\nSummary table showing optimal GCV and OCV values for each tested spline degree.\n\n\noptimal_gcv\ndata.frame\nSubset of results containing Œª values that minimize GCV for each degree.\n\n\noptimal_ocv\ndata.frame\nSubset of results containing Œª values that minimize OCV for each degree.\n\n\nplots\nlist\nList of ggplot2 objects showing evolution of df, sse, gcv, and ocv across Œª (if plot = TRUE).\n\n\nsummary_panel\ngrob\nCombined graphical summary of optimal smoothing parameters.\n\n\ndegree\nnumeric\nCurrent spline degree m.\n\n\npenalty_type\ncharacter\nPenalization type used.\n\n\ncall\ncall\nFunction call for reproducibility."
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#usage-examples",
    "href": "singleFunctions/smoothingSelection.html#usage-examples",
    "title": "smoothingSelection()",
    "section": "üí° Usage Examples",
    "text": "üí° Usage Examples\n\nBasic Usage\nlibrary(cccc)\n\n# Import and normalize data\ncorpus &lt;- importData(\"tdm.csv\", \"corpus_info.csv\")\ncorpus_norm &lt;- normalization(corpus, normty = \"nc\")\n\n# Find optimal smoothing parameters (default settings)\nsmooth_params &lt;- smoothingSelection(corpus_norm)\n\n# View optimal parameters\nsmooth_params$summary_optimal\n\n\nCustom Lambda Sequence\n# Use finer lambda sequence for more precision\nsmooth_params &lt;- smoothingSelection(\n  corpus_norm,\n  lambda_seq = seq(-6, 9, 0.1)  # Finer steps\n)\n\n\nSpecific Degree Range\n# Test only cubic and quartic splines\nsmooth_params &lt;- smoothingSelection(\n  corpus_norm,\n  degrees = 3:4\n)\n\n\nDifferent Penalty Type\n# Use second derivative penalty\nsmooth_params &lt;- smoothingSelection(\n  corpus_norm,\n  penalty_type = \"2\"\n)"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#interpreting-the-output",
    "href": "singleFunctions/smoothingSelection.html#interpreting-the-output",
    "title": "smoothingSelection()",
    "section": "üìä Interpreting the Output",
    "text": "üìä Interpreting the Output\n\n1. Summary Optimal Table\nsmooth_params$summary_optimal\nShows for each degree: - degree: Spline degree (m) - optimal_lambda_gcv: Œª that minimizes GCV - min_gcv: Minimum GCV value - optimal_lambda_ocv: Œª that minimizes OCV - min_ocv: Minimum OCV value\nHow to use: Choose the degree with the lowest GCV or OCV value.\n\n\n2. Diagnostic Plots\nFour plots are generated (if plot = TRUE):\n\nDegrees of Freedom (df) Plot\nShows effective model complexity across Œª values. - Low df: Highly smoothed (simple model) - High df: Less smoothed (complex model)\n\n\nSum of Squared Errors (SSE) Plot\nShows fit to the data. - Low SSE: Better fit to observed data - High SSE: Poorer fit (oversmoothed)\n\n\nGCV Plot\nShows estimated prediction error. - Minimum: Optimal balance point - U-shaped curve: Too much/too little smoothing both increase error\n\n\nOCV Plot\nShows true cross-validation error. - Similar interpretation to GCV - More reliable but computationally intensive\n\n\n\n3. Typical Pattern\n        GCV/OCV\n         |\n    high |    \\\n         |     \\___/  ‚Üê optimal\n         |         \\\n     low |__________\\___\n         small Œª  ‚Üí  large Œª\n         (rough)    (smooth)"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#choosing-parameters",
    "href": "singleFunctions/smoothingSelection.html#choosing-parameters",
    "title": "smoothingSelection()",
    "section": "üéØ Choosing Parameters",
    "text": "üéØ Choosing Parameters\n\nStep 1: Compare Degrees\n# Look at summary table\nsmooth_params$summary_optimal\n\n# Find degree with minimum GCV\nbest_degree &lt;- which.min(smooth_params$summary_optimal$min_gcv)\n\n\nStep 2: Extract Optimal Lambda\n# Get optimal lambda for best degree\noptimal_params &lt;- smooth_params$optimal_gcv %&gt;%\n  filter(degree == best_degree)\n\n\nStep 3: Validate with Plots\n\nCheck that GCV curve has clear minimum (U-shape)\nVerify df values are reasonable (not too high or too low)\nCompare GCV and OCV to ensure consistency"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#use-cases",
    "href": "singleFunctions/smoothingSelection.html#use-cases",
    "title": "smoothingSelection()",
    "section": "üìà Use Cases",
    "text": "üìà Use Cases\n\n1. Initial Analysis\nFirst time analyzing a corpus‚Äîneed to find appropriate smoothing level.\n\n\n2. Method Comparison\nTesting different penalty strategies to see which suits your data.\n\n\n3. Parameter Sensitivity\nUnderstanding how robust your results are to smoothing choices.\n\n\n4. Publication Preparation\nDocumenting systematic parameter selection for research papers.\n\n\n5. Multi-Corpus Studies\nFinding consistent parameters across different corpora."
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#tips-best-practices",
    "href": "singleFunctions/smoothingSelection.html#tips-best-practices",
    "title": "smoothingSelection()",
    "section": "üí° Tips & Best Practices",
    "text": "üí° Tips & Best Practices\n\nStart with defaults ‚Äî They work well for most corpora\nAlways check plots ‚Äî Visual inspection catches issues that numbers don‚Äôt\nCompare GCV and OCV ‚Äî Consistency indicates reliable results\nDon‚Äôt oversearch ‚Äî Finer lambda sequences rarely improve results significantly\nConsider computation time ‚Äî Balance precision with practical constraints\nDocument your choice ‚Äî Save summary_optimal table for methods section\nPrefer simpler models ‚Äî If two degrees are similar, choose the lower one\nUse normalized data ‚Äî Always normalize before smoothing selection"
  },
  {
    "objectID": "singleFunctions/smoothingSelection.html#see-also",
    "href": "singleFunctions/smoothingSelection.html#see-also",
    "title": "smoothingSelection()",
    "section": "üìö See Also",
    "text": "üìö See Also\n\nnormalization() ‚Äî Apply before smoothing selection\noptimalSmoothing() ‚Äî Next step: select final degree and penalty\nplotSuboptimalFits() ‚Äî Visualize different smoothing options\ncurvePlot() ‚Äî Visualize raw trajectories"
  }
]